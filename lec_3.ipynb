{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNyX2nrk1hHNyugyMaGsH8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sangyups/PyTorchZeroToAll/blob/main/lec_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqL38UxG8ITf",
        "outputId": "93a6bbd1-e9af-435d-adf1-9df4686002d5"
      },
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "  return w * x #y_hat\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) ** 2\n",
        "def gradient(x,y):\n",
        "  return 2 * x * (x * w - y)\n",
        "\n",
        "print('predict (before training)', 4, forward(4).data[0])\n",
        "\n",
        "for epoch in range(100):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    l = loss(x_val, y_val)\n",
        "    l.backward()\n",
        "    print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
        "    w.data = w.data - 0.01 * w.grad.data\n",
        "\n",
        "    w.grad.data.zero_()\n",
        "  print('progress:', epoch, l.data[0])\n",
        "print('predict (after training)', '4 hours', forward(4).data[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor(4.)\n",
            "\tgrad:  1.0 2.0 tensor(-2.)\n",
            "\tgrad:  2.0 4.0 tensor(-7.8400)\n",
            "\tgrad:  3.0 6.0 tensor(-16.2288)\n",
            "progress: 0 tensor(7.3159)\n",
            "\tgrad:  1.0 2.0 tensor(-1.4786)\n",
            "\tgrad:  2.0 4.0 tensor(-5.7962)\n",
            "\tgrad:  3.0 6.0 tensor(-11.9981)\n",
            "progress: 1 tensor(3.9988)\n",
            "\tgrad:  1.0 2.0 tensor(-1.0932)\n",
            "\tgrad:  2.0 4.0 tensor(-4.2852)\n",
            "\tgrad:  3.0 6.0 tensor(-8.8704)\n",
            "progress: 2 tensor(2.1857)\n",
            "\tgrad:  1.0 2.0 tensor(-0.8082)\n",
            "\tgrad:  2.0 4.0 tensor(-3.1681)\n",
            "\tgrad:  3.0 6.0 tensor(-6.5580)\n",
            "progress: 3 tensor(1.1946)\n",
            "\tgrad:  1.0 2.0 tensor(-0.5975)\n",
            "\tgrad:  2.0 4.0 tensor(-2.3422)\n",
            "\tgrad:  3.0 6.0 tensor(-4.8484)\n",
            "progress: 4 tensor(0.6530)\n",
            "\tgrad:  1.0 2.0 tensor(-0.4417)\n",
            "\tgrad:  2.0 4.0 tensor(-1.7316)\n",
            "\tgrad:  3.0 6.0 tensor(-3.5845)\n",
            "progress: 5 tensor(0.3569)\n",
            "\tgrad:  1.0 2.0 tensor(-0.3266)\n",
            "\tgrad:  2.0 4.0 tensor(-1.2802)\n",
            "\tgrad:  3.0 6.0 tensor(-2.6500)\n",
            "progress: 6 tensor(0.1951)\n",
            "\tgrad:  1.0 2.0 tensor(-0.2414)\n",
            "\tgrad:  2.0 4.0 tensor(-0.9465)\n",
            "\tgrad:  3.0 6.0 tensor(-1.9592)\n",
            "progress: 7 tensor(0.1066)\n",
            "\tgrad:  1.0 2.0 tensor(-0.1785)\n",
            "\tgrad:  2.0 4.0 tensor(-0.6997)\n",
            "\tgrad:  3.0 6.0 tensor(-1.4485)\n",
            "progress: 8 tensor(0.0583)\n",
            "\tgrad:  1.0 2.0 tensor(-0.1320)\n",
            "\tgrad:  2.0 4.0 tensor(-0.5173)\n",
            "\tgrad:  3.0 6.0 tensor(-1.0709)\n",
            "progress: 9 tensor(0.0319)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0976)\n",
            "\tgrad:  2.0 4.0 tensor(-0.3825)\n",
            "\tgrad:  3.0 6.0 tensor(-0.7917)\n",
            "progress: 10 tensor(0.0174)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0721)\n",
            "\tgrad:  2.0 4.0 tensor(-0.2828)\n",
            "\tgrad:  3.0 6.0 tensor(-0.5853)\n",
            "progress: 11 tensor(0.0095)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0533)\n",
            "\tgrad:  2.0 4.0 tensor(-0.2090)\n",
            "\tgrad:  3.0 6.0 tensor(-0.4327)\n",
            "progress: 12 tensor(0.0052)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0394)\n",
            "\tgrad:  2.0 4.0 tensor(-0.1546)\n",
            "\tgrad:  3.0 6.0 tensor(-0.3199)\n",
            "progress: 13 tensor(0.0028)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0291)\n",
            "\tgrad:  2.0 4.0 tensor(-0.1143)\n",
            "\tgrad:  3.0 6.0 tensor(-0.2365)\n",
            "progress: 14 tensor(0.0016)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0215)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0845)\n",
            "\tgrad:  3.0 6.0 tensor(-0.1749)\n",
            "progress: 15 tensor(0.0008)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0159)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0625)\n",
            "\tgrad:  3.0 6.0 tensor(-0.1293)\n",
            "progress: 16 tensor(0.0005)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0118)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0462)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0956)\n",
            "progress: 17 tensor(0.0003)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0087)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0341)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0707)\n",
            "progress: 18 tensor(0.0001)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0064)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0252)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0522)\n",
            "progress: 19 tensor(7.5804e-05)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0048)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0187)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0386)\n",
            "progress: 20 tensor(4.1433e-05)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0035)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0138)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0286)\n",
            "progress: 21 tensor(2.2647e-05)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0026)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0102)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0211)\n",
            "progress: 22 tensor(1.2377e-05)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0019)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0075)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0156)\n",
            "progress: 23 tensor(6.7684e-06)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0014)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0056)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0115)\n",
            "progress: 24 tensor(3.7001e-06)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0011)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0041)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0085)\n",
            "progress: 25 tensor(2.0219e-06)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0008)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0030)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0063)\n",
            "progress: 26 tensor(1.1045e-06)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0006)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0023)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0047)\n",
            "progress: 27 tensor(6.0411e-07)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0004)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0017)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0034)\n",
            "progress: 28 tensor(3.2960e-07)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0003)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0012)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0025)\n",
            "progress: 29 tensor(1.8051e-07)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0002)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0009)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0019)\n",
            "progress: 30 tensor(9.8744e-08)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0002)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0007)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0014)\n",
            "progress: 31 tensor(5.4148e-08)\n",
            "\tgrad:  1.0 2.0 tensor(-0.0001)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0005)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0010)\n",
            "progress: 32 tensor(2.9468e-08)\n",
            "\tgrad:  1.0 2.0 tensor(-9.3937e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0004)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0008)\n",
            "progress: 33 tensor(1.6088e-08)\n",
            "\tgrad:  1.0 2.0 tensor(-6.9380e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0003)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0006)\n",
            "progress: 34 tensor(8.7348e-09)\n",
            "\tgrad:  1.0 2.0 tensor(-5.1260e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0002)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0004)\n",
            "progress: 35 tensor(4.8467e-09)\n",
            "\tgrad:  1.0 2.0 tensor(-3.7909e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0001)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0003)\n",
            "progress: 36 tensor(2.6521e-09)\n",
            "\tgrad:  1.0 2.0 tensor(-2.8133e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0001)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0002)\n",
            "progress: 37 tensor(1.4552e-09)\n",
            "\tgrad:  1.0 2.0 tensor(-2.0981e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-8.2016e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0002)\n",
            "progress: 38 tensor(7.9149e-10)\n",
            "\tgrad:  1.0 2.0 tensor(-1.5497e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-6.1035e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0001)\n",
            "progress: 39 tensor(4.4020e-10)\n",
            "\tgrad:  1.0 2.0 tensor(-1.1444e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-4.4823e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-9.1553e-05)\n",
            "progress: 40 tensor(2.3283e-10)\n",
            "\tgrad:  1.0 2.0 tensor(-8.3447e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-3.2425e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-6.5804e-05)\n",
            "progress: 41 tensor(1.2028e-10)\n",
            "\tgrad:  1.0 2.0 tensor(-5.9605e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-2.2888e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-4.5776e-05)\n",
            "progress: 42 tensor(5.8208e-11)\n",
            "\tgrad:  1.0 2.0 tensor(-4.2915e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-1.7166e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-3.7193e-05)\n",
            "progress: 43 tensor(3.8426e-11)\n",
            "\tgrad:  1.0 2.0 tensor(-3.3379e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-1.3351e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-2.8610e-05)\n",
            "progress: 44 tensor(2.2737e-11)\n",
            "\tgrad:  1.0 2.0 tensor(-2.6226e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-1.0490e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-2.2888e-05)\n",
            "progress: 45 tensor(1.4552e-11)\n",
            "\tgrad:  1.0 2.0 tensor(-1.9073e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-7.6294e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-1.4305e-05)\n",
            "progress: 46 tensor(5.6843e-12)\n",
            "\tgrad:  1.0 2.0 tensor(-1.4305e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-5.7220e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-1.1444e-05)\n",
            "progress: 47 tensor(3.6380e-12)\n",
            "\tgrad:  1.0 2.0 tensor(-1.1921e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-4.7684e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-1.1444e-05)\n",
            "progress: 48 tensor(3.6380e-12)\n",
            "\tgrad:  1.0 2.0 tensor(-9.5367e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-3.8147e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-8.5831e-06)\n",
            "progress: 49 tensor(2.0464e-12)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 50 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 51 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 52 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 53 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 54 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 55 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 56 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 57 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 58 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 59 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 60 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 61 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 62 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 63 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 64 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 65 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 66 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 67 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 68 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 69 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 70 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 71 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 72 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 73 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 74 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 75 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 76 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 77 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 78 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 79 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 80 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 81 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 82 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 83 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 84 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 85 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 86 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 87 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 88 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 89 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 90 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 91 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 92 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 93 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 94 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 95 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 96 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 97 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 98 tensor(9.0949e-13)\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "progress: 99 tensor(9.0949e-13)\n",
            "predict (after training) 4 hours tensor(8.0000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYg2k9-DCWdY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "45e495c5-56b4-48eb-eb1e-79373e93c1a6"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = Variable(torch.Tensor([-3.0]), requires_grad=True)\n",
        "w_list = []\n",
        "l_list = []\n",
        "\n",
        "def forward(x):\n",
        "  return w * x #y_hat\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) ** 2\n",
        "def gradient(x,y):\n",
        "  return 2 * x * (x * w - y)\n",
        "\n",
        "print('predict (before training)', 4, forward(4).data[0])\n",
        "\n",
        "for epoch in range(100):\n",
        "  l_sum = 0\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    l = loss(x_val, y_val)\n",
        "    l_sum += l\n",
        "    l.backward()\n",
        "    print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
        "    w.data = w.data - 0.01 * w.grad.data\n",
        "\n",
        "    w.grad.data.zero_()\n",
        "  l_list.append(l_sum / 3)\n",
        "  w_list.append(w.data[0])\n",
        "  print('progress:', epoch, l.item())\n",
        "print('predict (after training)', '4 hours', forward(4).item())\n",
        "plt.plot(w_list, l_list)\n",
        "plt.xlabel('weight')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor(-12.)\n",
            "\tgrad:  1.0 2.0 -10.0\n",
            "\tgrad:  2.0 4.0 -39.20000076293945\n",
            "\tgrad:  3.0 6.0 -81.14399719238281\n",
            "progress: 0 182.89857482910156\n",
            "\tgrad:  1.0 2.0 -7.393120288848877\n",
            "\tgrad:  2.0 4.0 -28.98103141784668\n",
            "\tgrad:  3.0 6.0 -59.99073791503906\n",
            "progress: 1 99.96912384033203\n",
            "\tgrad:  1.0 2.0 -5.465822696685791\n",
            "\tgrad:  2.0 4.0 -21.426025390625\n",
            "\tgrad:  3.0 6.0 -44.35186767578125\n",
            "progress: 2 54.64134216308594\n",
            "\tgrad:  1.0 2.0 -4.040948390960693\n",
            "\tgrad:  2.0 4.0 -15.840517044067383\n",
            "\tgrad:  3.0 6.0 -32.78987121582031\n",
            "progress: 3 29.86598777770996\n",
            "\tgrad:  1.0 2.0 -2.9875216484069824\n",
            "\tgrad:  2.0 4.0 -11.711084365844727\n",
            "\tgrad:  3.0 6.0 -24.241945266723633\n",
            "progress: 4 16.324220657348633\n",
            "\tgrad:  1.0 2.0 -2.2087104320526123\n",
            "\tgrad:  2.0 4.0 -8.6581449508667\n",
            "\tgrad:  3.0 6.0 -17.922359466552734\n",
            "progress: 5 8.922528266906738\n",
            "\tgrad:  1.0 2.0 -1.6329259872436523\n",
            "\tgrad:  2.0 4.0 -6.401069641113281\n",
            "\tgrad:  3.0 6.0 -13.250212669372559\n",
            "progress: 6 4.876892566680908\n",
            "\tgrad:  1.0 2.0 -1.2072417736053467\n",
            "\tgrad:  2.0 4.0 -4.732387542724609\n",
            "\tgrad:  3.0 6.0 -9.796042442321777\n",
            "progress: 7 2.665623426437378\n",
            "\tgrad:  1.0 2.0 -0.8925282955169678\n",
            "\tgrad:  2.0 4.0 -3.4987106323242188\n",
            "\tgrad:  3.0 6.0 -7.242330551147461\n",
            "progress: 8 1.4569820165634155\n",
            "\tgrad:  1.0 2.0 -0.6598567962646484\n",
            "\tgrad:  2.0 4.0 -2.5866384506225586\n",
            "\tgrad:  3.0 6.0 -5.354341506958008\n",
            "progress: 9 0.7963603734970093\n",
            "\tgrad:  1.0 2.0 -0.487839937210083\n",
            "\tgrad:  2.0 4.0 -1.912332534790039\n",
            "\tgrad:  3.0 6.0 -3.958528518676758\n",
            "progress: 10 0.4352763295173645\n",
            "\tgrad:  1.0 2.0 -0.36066603660583496\n",
            "\tgrad:  2.0 4.0 -1.4138107299804688\n",
            "\tgrad:  3.0 6.0 -2.926589012145996\n",
            "progress: 11 0.2379145324230194\n",
            "\tgrad:  1.0 2.0 -0.2666447162628174\n",
            "\tgrad:  2.0 4.0 -1.0452470779418945\n",
            "\tgrad:  3.0 6.0 -2.1636600494384766\n",
            "progress: 12 0.13003957271575928\n",
            "\tgrad:  1.0 2.0 -0.19713354110717773\n",
            "\tgrad:  2.0 4.0 -0.7727632522583008\n",
            "\tgrad:  3.0 6.0 -1.5996208190917969\n",
            "progress: 13 0.07107741385698318\n",
            "\tgrad:  1.0 2.0 -0.14574313163757324\n",
            "\tgrad:  2.0 4.0 -0.5713129043579102\n",
            "\tgrad:  3.0 6.0 -1.1826181411743164\n",
            "progress: 14 0.0388496033847332\n",
            "\tgrad:  1.0 2.0 -0.10774970054626465\n",
            "\tgrad:  2.0 4.0 -0.4223785400390625\n",
            "\tgrad:  3.0 6.0 -0.8743228912353516\n",
            "progress: 15 0.021234458312392235\n",
            "\tgrad:  1.0 2.0 -0.07966041564941406\n",
            "\tgrad:  2.0 4.0 -0.3122692108154297\n",
            "\tgrad:  3.0 6.0 -0.6463966369628906\n",
            "progress: 16 0.011606350541114807\n",
            "\tgrad:  1.0 2.0 -0.05889391899108887\n",
            "\tgrad:  2.0 4.0 -0.2308645248413086\n",
            "\tgrad:  3.0 6.0 -0.4778909683227539\n",
            "progress: 17 0.006343882530927658\n",
            "\tgrad:  1.0 2.0 -0.04354119300842285\n",
            "\tgrad:  2.0 4.0 -0.17068099975585938\n",
            "\tgrad:  3.0 6.0 -0.35330772399902344\n",
            "progress: 18 0.003467398462817073\n",
            "\tgrad:  1.0 2.0 -0.03219032287597656\n",
            "\tgrad:  2.0 4.0 -0.12618637084960938\n",
            "\tgrad:  3.0 6.0 -0.26120567321777344\n",
            "progress: 19 0.0018952334066852927\n",
            "\tgrad:  1.0 2.0 -0.023798704147338867\n",
            "\tgrad:  2.0 4.0 -0.0932912826538086\n",
            "\tgrad:  3.0 6.0 -0.1931133270263672\n",
            "progress: 20 0.001035909866914153\n",
            "\tgrad:  1.0 2.0 -0.017594575881958008\n",
            "\tgrad:  2.0 4.0 -0.0689706802368164\n",
            "\tgrad:  3.0 6.0 -0.14276790618896484\n",
            "progress: 21 0.0005661853938363492\n",
            "\tgrad:  1.0 2.0 -0.013007879257202148\n",
            "\tgrad:  2.0 4.0 -0.050991058349609375\n",
            "\tgrad:  3.0 6.0 -0.10555171966552734\n",
            "progress: 22 0.0003094768326263875\n",
            "\tgrad:  1.0 2.0 -0.009617090225219727\n",
            "\tgrad:  2.0 4.0 -0.03769874572753906\n",
            "\tgrad:  3.0 6.0 -0.07803726196289062\n",
            "progress: 23 0.00016916150343604386\n",
            "\tgrad:  1.0 2.0 -0.007110118865966797\n",
            "\tgrad:  2.0 4.0 -0.027872085571289062\n",
            "\tgrad:  3.0 6.0 -0.05769538879394531\n",
            "progress: 24 9.24654959817417e-05\n",
            "\tgrad:  1.0 2.0 -0.00525665283203125\n",
            "\tgrad:  2.0 4.0 -0.020606040954589844\n",
            "\tgrad:  3.0 6.0 -0.042652130126953125\n",
            "progress: 25 5.0533450121292844e-05\n",
            "\tgrad:  1.0 2.0 -0.0038862228393554688\n",
            "\tgrad:  2.0 4.0 -0.015233993530273438\n",
            "\tgrad:  3.0 6.0 -0.03153419494628906\n",
            "progress: 26 2.7622372726909816e-05\n",
            "\tgrad:  1.0 2.0 -0.0028731822967529297\n",
            "\tgrad:  2.0 4.0 -0.011262893676757812\n",
            "\tgrad:  3.0 6.0 -0.023314476013183594\n",
            "progress: 27 1.5099021766218357e-05\n",
            "\tgrad:  1.0 2.0 -0.0021240711212158203\n",
            "\tgrad:  2.0 4.0 -0.008326530456542969\n",
            "\tgrad:  3.0 6.0 -0.01723766326904297\n",
            "progress: 28 8.2538062997628e-06\n",
            "\tgrad:  1.0 2.0 -0.0015704631805419922\n",
            "\tgrad:  2.0 4.0 -0.006155967712402344\n",
            "\tgrad:  3.0 6.0 -0.012742996215820312\n",
            "progress: 29 4.510665348789189e-06\n",
            "\tgrad:  1.0 2.0 -0.0011610984802246094\n",
            "\tgrad:  2.0 4.0 -0.004551887512207031\n",
            "\tgrad:  3.0 6.0 -0.009421348571777344\n",
            "progress: 30 2.4656058030814165e-06\n",
            "\tgrad:  1.0 2.0 -0.0008585453033447266\n",
            "\tgrad:  2.0 4.0 -0.0033655166625976562\n",
            "\tgrad:  3.0 6.0 -0.006966590881347656\n",
            "progress: 31 1.3481496807798976e-06\n",
            "\tgrad:  1.0 2.0 -0.0006349086761474609\n",
            "\tgrad:  2.0 4.0 -0.0024890899658203125\n",
            "\tgrad:  3.0 6.0 -0.005152702331542969\n",
            "progress: 32 7.37509481041343e-07\n",
            "\tgrad:  1.0 2.0 -0.00046944618225097656\n",
            "\tgrad:  2.0 4.0 -0.0018405914306640625\n",
            "\tgrad:  3.0 6.0 -0.003810882568359375\n",
            "progress: 33 4.0341183193959296e-07\n",
            "\tgrad:  1.0 2.0 -0.000347137451171875\n",
            "\tgrad:  2.0 4.0 -0.0013608932495117188\n",
            "\tgrad:  3.0 6.0 -0.0028181076049804688\n",
            "progress: 34 2.2060362425690982e-07\n",
            "\tgrad:  1.0 2.0 -0.0002567768096923828\n",
            "\tgrad:  2.0 4.0 -0.0010061264038085938\n",
            "\tgrad:  3.0 6.0 -0.00208282470703125\n",
            "progress: 35 1.205044100061059e-07\n",
            "\tgrad:  1.0 2.0 -0.00018978118896484375\n",
            "\tgrad:  2.0 4.0 -0.000743865966796875\n",
            "\tgrad:  3.0 6.0 -0.0015392303466796875\n",
            "progress: 36 6.581194611499086e-08\n",
            "\tgrad:  1.0 2.0 -0.0001404285430908203\n",
            "\tgrad:  2.0 4.0 -0.0005502700805664062\n",
            "\tgrad:  3.0 6.0 -0.0011386871337890625\n",
            "progress: 37 3.601689968490973e-08\n",
            "\tgrad:  1.0 2.0 -0.00010371208190917969\n",
            "\tgrad:  2.0 4.0 -0.0004062652587890625\n",
            "\tgrad:  3.0 6.0 -0.0008411407470703125\n",
            "progress: 38 1.9653271010611206e-08\n",
            "\tgrad:  1.0 2.0 -7.653236389160156e-05\n",
            "\tgrad:  2.0 4.0 -0.00030040740966796875\n",
            "\tgrad:  3.0 6.0 -0.0006237030029296875\n",
            "progress: 39 1.080570655176416e-08\n",
            "\tgrad:  1.0 2.0 -5.6743621826171875e-05\n",
            "\tgrad:  2.0 4.0 -0.00022220611572265625\n",
            "\tgrad:  3.0 6.0 -0.000457763671875\n",
            "progress: 40 5.820766091346741e-09\n",
            "\tgrad:  1.0 2.0 -4.1961669921875e-05\n",
            "\tgrad:  2.0 4.0 -0.000164031982421875\n",
            "\tgrad:  3.0 6.0 -0.0003376007080078125\n",
            "progress: 41 3.165951056871563e-09\n",
            "\tgrad:  1.0 2.0 -3.0994415283203125e-05\n",
            "\tgrad:  2.0 4.0 -0.00012111663818359375\n",
            "\tgrad:  3.0 6.0 -0.00025177001953125\n",
            "progress: 42 1.760781742632389e-09\n",
            "\tgrad:  1.0 2.0 -2.288818359375e-05\n",
            "\tgrad:  2.0 4.0 -8.96453857421875e-05\n",
            "\tgrad:  3.0 6.0 -0.00018310546875\n",
            "progress: 43 9.313225746154785e-10\n",
            "\tgrad:  1.0 2.0 -1.6927719116210938e-05\n",
            "\tgrad:  2.0 4.0 -6.67572021484375e-05\n",
            "\tgrad:  3.0 6.0 -0.0001373291015625\n",
            "progress: 44 5.238689482212067e-10\n",
            "\tgrad:  1.0 2.0 -1.239776611328125e-05\n",
            "\tgrad:  2.0 4.0 -4.863739013671875e-05\n",
            "\tgrad:  3.0 6.0 -0.00010013580322265625\n",
            "progress: 45 2.7853275241795927e-10\n",
            "\tgrad:  1.0 2.0 -9.298324584960938e-06\n",
            "\tgrad:  2.0 4.0 -3.62396240234375e-05\n",
            "\tgrad:  3.0 6.0 -7.43865966796875e-05\n",
            "progress: 46 1.5370460459962487e-10\n",
            "\tgrad:  1.0 2.0 -6.9141387939453125e-06\n",
            "\tgrad:  2.0 4.0 -2.6702880859375e-05\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-05\n",
            "progress: 47 9.094947017729282e-11\n",
            "\tgrad:  1.0 2.0 -5.0067901611328125e-06\n",
            "\tgrad:  2.0 4.0 -2.002716064453125e-05\n",
            "\tgrad:  3.0 6.0 -4.00543212890625e-05\n",
            "progress: 48 4.4565240386873484e-11\n",
            "\tgrad:  1.0 2.0 -3.814697265625e-06\n",
            "\tgrad:  2.0 4.0 -1.52587890625e-05\n",
            "\tgrad:  3.0 6.0 -3.147125244140625e-05\n",
            "progress: 49 2.751221472863108e-11\n",
            "\tgrad:  1.0 2.0 -2.86102294921875e-06\n",
            "\tgrad:  2.0 4.0 -1.1444091796875e-05\n",
            "\tgrad:  3.0 6.0 -2.288818359375e-05\n",
            "progress: 50 1.4551915228366852e-11\n",
            "\tgrad:  1.0 2.0 -2.1457672119140625e-06\n",
            "\tgrad:  2.0 4.0 -8.58306884765625e-06\n",
            "\tgrad:  3.0 6.0 -1.71661376953125e-05\n",
            "progress: 51 8.185452315956354e-12\n",
            "\tgrad:  1.0 2.0 -1.6689300537109375e-06\n",
            "\tgrad:  2.0 4.0 -6.67572021484375e-06\n",
            "\tgrad:  3.0 6.0 -1.1444091796875e-05\n",
            "progress: 52 3.637978807091713e-12\n",
            "\tgrad:  1.0 2.0 -1.1920928955078125e-06\n",
            "\tgrad:  2.0 4.0 -4.76837158203125e-06\n",
            "\tgrad:  3.0 6.0 -1.1444091796875e-05\n",
            "progress: 53 3.637978807091713e-12\n",
            "\tgrad:  1.0 2.0 -9.5367431640625e-07\n",
            "\tgrad:  2.0 4.0 -3.814697265625e-06\n",
            "\tgrad:  3.0 6.0 -8.58306884765625e-06\n",
            "progress: 54 2.0463630789890885e-12\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 55 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 56 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 57 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 58 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 59 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 60 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 61 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 62 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 63 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 64 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 65 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 66 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 67 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 68 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 69 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 70 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 71 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 72 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 73 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 74 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 75 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 76 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 77 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 78 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 79 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 80 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 81 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 82 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 83 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 84 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 85 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 86 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 87 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 88 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 89 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 90 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 91 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 92 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 93 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 94 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 95 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 96 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 97 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 98 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 -7.152557373046875e-07\n",
            "\tgrad:  2.0 4.0 -2.86102294921875e-06\n",
            "\tgrad:  3.0 6.0 -5.7220458984375e-06\n",
            "progress: 99 9.094947017729282e-13\n",
            "predict (after training) 4 hours 7.999998569488525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhjcemlGGemv",
        "outputId": "f9656fb3-edd4-4ecc-efa9-21c77a57a933"
      },
      "source": [
        "#what if y = x^2 * 2 + x * 4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [6.0, 16.0, 30.0]\n",
        "\n",
        "w1 = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
        "w2 = Variable(torch.Tensor([1.0]), requires_grad=True)\n",
        "\n",
        "def forward(x):\n",
        "  return w2 * (x**2) + w1 * x #y_hat\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) ** 2\n",
        "\n",
        "print('predict (before training)', 4, forward(4).data[0])\n",
        "\n",
        "for epoch in range(200):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    l = loss(x_val, y_val)\n",
        "    l.backward()\n",
        "    print(\"\\tgrad: \", x_val, y_val, w1.grad.data[0], w2.grad.data[0])\n",
        "    print('\\t', w1.data[0], w2.data[0])\n",
        "    w1.data = w1.data - 0.02 * w1.grad.data\n",
        "    w2.data = w2.data - 0.01 * w2.grad.data\n",
        "    w1.grad.data.zero_()\n",
        "    w2.grad.data.zero_()\n",
        "  print('progress:', epoch, l.data[0])\n",
        "\n",
        "print('predict (after training)', '4 hours', forward(4).data[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor(20.)\n",
            "\tgrad:  1.0 6.0 tensor(-8.) tensor(-8.)\n",
            "\t tensor(1.) tensor(1.)\n",
            "\tgrad:  2.0 16.0 tensor(-37.4400) tensor(-74.8800)\n",
            "\t tensor(1.1600) tensor(1.0800)\n",
            "\tgrad:  3.0 30.0 tensor(-46.8864) tensor(-140.6592)\n",
            "\t tensor(1.9088) tensor(1.8288)\n",
            "progress: 0 tensor(61.0648)\n",
            "\tgrad:  1.0 6.0 tensor(0.1638) tensor(0.1638)\n",
            "\t tensor(2.8465) tensor(3.2354)\n",
            "\tgrad:  2.0 16.0 tensor(10.4861) tensor(20.9721)\n",
            "\t tensor(2.8433) tensor(3.2338)\n",
            "\tgrad:  3.0 30.0 tensor(30.7013) tensor(92.1038)\n",
            "\t tensor(2.6335) tensor(3.0240)\n",
            "progress: 1 tensor(26.1824)\n",
            "\tgrad:  1.0 6.0 tensor(-3.7550) tensor(-3.7550)\n",
            "\t tensor(2.0195) tensor(2.1030)\n",
            "\tgrad:  2.0 16.0 tensor(-12.9945) tensor(-25.9889)\n",
            "\t tensor(2.0946) tensor(2.1405)\n",
            "\tgrad:  3.0 30.0 tensor(-7.9957) tensor(-23.9872)\n",
            "\t tensor(2.3545) tensor(2.4004)\n",
            "progress: 2 tensor(1.7759)\n",
            "\tgrad:  1.0 6.0 tensor(-1.6906) tensor(-1.6906)\n",
            "\t tensor(2.5144) tensor(2.6403)\n",
            "\tgrad:  2.0 16.0 tensor(-1.0989) tensor(-2.1977)\n",
            "\t tensor(2.5482) tensor(2.6572)\n",
            "\tgrad:  3.0 30.0 tensor(10.9397) tensor(32.8191)\n",
            "\t tensor(2.5702) tensor(2.6792)\n",
            "progress: 3 tensor(3.3244)\n",
            "\tgrad:  1.0 6.0 tensor(-2.5952) tensor(-2.5952)\n",
            "\t tensor(2.3514) tensor(2.3510)\n",
            "\tgrad:  2.0 16.0 tensor(-6.7424) tensor(-13.4847)\n",
            "\t tensor(2.4033) tensor(2.3769)\n",
            "\tgrad:  3.0 30.0 tensor(1.3238) tensor(3.9714)\n",
            "\t tensor(2.5382) tensor(2.5118)\n",
            "progress: 4 tensor(0.0487)\n",
            "\tgrad:  1.0 6.0 tensor(-2.0325) tensor(-2.0325)\n",
            "\t tensor(2.5117) tensor(2.4721)\n",
            "\tgrad:  2.0 16.0 tensor(-3.7029) tensor(-7.4057)\n",
            "\t tensor(2.5523) tensor(2.4924)\n",
            "\tgrad:  3.0 30.0 tensor(5.8640) tensor(17.5921)\n",
            "\t tensor(2.6264) tensor(2.5665)\n",
            "progress: 5 tensor(0.9552)\n",
            "\tgrad:  1.0 6.0 tensor(-2.2007) tensor(-2.2007)\n",
            "\t tensor(2.5091) tensor(2.3905)\n",
            "\tgrad:  2.0 16.0 tensor(-4.9742) tensor(-9.9485)\n",
            "\t tensor(2.5531) tensor(2.4126)\n",
            "\tgrad:  3.0 30.0 tensor(3.3968) tensor(10.1904)\n",
            "\t tensor(2.6526) tensor(2.5120)\n",
            "progress: 6 tensor(0.3205)\n",
            "\tgrad:  1.0 6.0 tensor(-2.0104) tensor(-2.0104)\n",
            "\t tensor(2.5847) tensor(2.4101)\n",
            "\tgrad:  2.0 16.0 tensor(-4.1172) tensor(-8.2344)\n",
            "\t tensor(2.6249) tensor(2.4302)\n",
            "\tgrad:  3.0 30.0 tensor(4.4093) tensor(13.2279)\n",
            "\t tensor(2.7072) tensor(2.5126)\n",
            "progress: 7 tensor(0.5401)\n",
            "\tgrad:  1.0 6.0 tensor(-2.0013) tensor(-2.0013)\n",
            "\t tensor(2.6190) tensor(2.3803)\n",
            "\tgrad:  2.0 16.0 tensor(-4.3225) tensor(-8.6450)\n",
            "\t tensor(2.6591) tensor(2.4003)\n",
            "\tgrad:  3.0 30.0 tensor(3.7045) tensor(11.1134)\n",
            "\t tensor(2.7455) tensor(2.4868)\n",
            "progress: 8 tensor(0.3812)\n",
            "\tgrad:  1.0 6.0 tensor(-1.9059) tensor(-1.9059)\n",
            "\t tensor(2.6714) tensor(2.3756)\n",
            "\tgrad:  2.0 16.0 tensor(-4.0087) tensor(-8.0173)\n",
            "\t tensor(2.7095) tensor(2.3947)\n",
            "\tgrad:  3.0 30.0 tensor(3.8574) tensor(11.5722)\n",
            "\t tensor(2.7897) tensor(2.4749)\n",
            "progress: 9 tensor(0.4133)\n",
            "\tgrad:  1.0 6.0 tensor(-1.8566) tensor(-1.8566)\n",
            "\t tensor(2.7126) tensor(2.3591)\n",
            "\tgrad:  2.0 16.0 tensor(-3.9591) tensor(-7.9183)\n",
            "\t tensor(2.7497) tensor(2.3777)\n",
            "\tgrad:  3.0 30.0 tensor(3.5918) tensor(10.7755)\n",
            "\t tensor(2.8289) tensor(2.4569)\n",
            "progress: 10 tensor(0.3584)\n",
            "\tgrad:  1.0 6.0 tensor(-1.7876) tensor(-1.7876)\n",
            "\t tensor(2.7570) tensor(2.3491)\n",
            "\tgrad:  2.0 16.0 tensor(-3.7855) tensor(-7.5709)\n",
            "\t tensor(2.7928) tensor(2.3670)\n",
            "\tgrad:  3.0 30.0 tensor(3.5400) tensor(10.6199)\n",
            "\t tensor(2.8685) tensor(2.4427)\n",
            "progress: 11 tensor(0.3481)\n",
            "\tgrad:  1.0 6.0 tensor(-1.7315) tensor(-1.7315)\n",
            "\t tensor(2.7977) tensor(2.3365)\n",
            "\tgrad:  2.0 16.0 tensor(-3.6799) tensor(-7.3598)\n",
            "\t tensor(2.8323) tensor(2.3538)\n",
            "\tgrad:  3.0 30.0 tensor(3.3883) tensor(10.1650)\n",
            "\t tensor(2.9059) tensor(2.4274)\n",
            "progress: 12 tensor(0.3189)\n",
            "\tgrad:  1.0 6.0 tensor(-1.6721) tensor(-1.6721)\n",
            "\t tensor(2.8382) tensor(2.3258)\n",
            "\tgrad:  2.0 16.0 tensor(-3.5470) tensor(-7.0940)\n",
            "\t tensor(2.8716) tensor(2.3425)\n",
            "\tgrad:  3.0 30.0 tensor(3.2920) tensor(9.8761)\n",
            "\t tensor(2.9426) tensor(2.4134)\n",
            "progress: 13 tensor(0.3010)\n",
            "\tgrad:  1.0 6.0 tensor(-1.6172) tensor(-1.6172)\n",
            "\t tensor(2.8767) tensor(2.3147)\n",
            "\tgrad:  2.0 16.0 tensor(-3.4338) tensor(-6.8677)\n",
            "\t tensor(2.9091) tensor(2.3309)\n",
            "\tgrad:  3.0 30.0 tensor(3.1740) tensor(9.5221)\n",
            "\t tensor(2.9777) tensor(2.3995)\n",
            "progress: 14 tensor(0.2798)\n",
            "\tgrad:  1.0 6.0 tensor(-1.5629) tensor(-1.5629)\n",
            "\t tensor(2.9142) tensor(2.3043)\n",
            "\tgrad:  2.0 16.0 tensor(-3.3169) tensor(-6.6337)\n",
            "\t tensor(2.9455) tensor(2.3199)\n",
            "\tgrad:  3.0 30.0 tensor(3.0723) tensor(9.2169)\n",
            "\t tensor(3.0118) tensor(2.3863)\n",
            "progress: 15 tensor(0.2622)\n",
            "\tgrad:  1.0 6.0 tensor(-1.5110) tensor(-1.5110)\n",
            "\t tensor(2.9504) tensor(2.2941)\n",
            "\tgrad:  2.0 16.0 tensor(-3.2075) tensor(-6.4151)\n",
            "\t tensor(2.9806) tensor(2.3092)\n",
            "\tgrad:  3.0 30.0 tensor(2.9679) tensor(8.9036)\n",
            "\t tensor(3.0448) tensor(2.3734)\n",
            "progress: 16 tensor(0.2447)\n",
            "\tgrad:  1.0 6.0 tensor(-1.4605) tensor(-1.4605)\n",
            "\t tensor(2.9854) tensor(2.2843)\n",
            "\tgrad:  2.0 16.0 tensor(-3.1000) tensor(-6.2000)\n",
            "\t tensor(3.0146) tensor(2.2989)\n",
            "\tgrad:  3.0 30.0 tensor(2.8699) tensor(8.6098)\n",
            "\t tensor(3.0766) tensor(2.3609)\n",
            "progress: 17 tensor(0.2288)\n",
            "\tgrad:  1.0 6.0 tensor(-1.4119) tensor(-1.4119)\n",
            "\t tensor(3.0192) tensor(2.2748)\n",
            "\tgrad:  2.0 16.0 tensor(-2.9969) tensor(-5.9939)\n",
            "\t tensor(3.0475) tensor(2.2890)\n",
            "\tgrad:  3.0 30.0 tensor(2.7738) tensor(8.3213)\n",
            "\t tensor(3.1074) tensor(2.3489)\n",
            "progress: 18 tensor(0.2137)\n",
            "\tgrad:  1.0 6.0 tensor(-1.3648) tensor(-1.3648)\n",
            "\t tensor(3.0519) tensor(2.2657)\n",
            "\tgrad:  2.0 16.0 tensor(-2.8969) tensor(-5.7938)\n",
            "\t tensor(3.0792) tensor(2.2793)\n",
            "\tgrad:  3.0 30.0 tensor(2.6815) tensor(8.0446)\n",
            "\t tensor(3.1372) tensor(2.3373)\n",
            "progress: 19 tensor(0.1997)\n",
            "\tgrad:  1.0 6.0 tensor(-1.3193) tensor(-1.3193)\n",
            "\t tensor(3.0835) tensor(2.2568)\n",
            "\tgrad:  2.0 16.0 tensor(-2.8004) tensor(-5.6008)\n",
            "\t tensor(3.1099) tensor(2.2700)\n",
            "\tgrad:  3.0 30.0 tensor(2.5920) tensor(7.7761)\n",
            "\t tensor(3.1659) tensor(2.3260)\n",
            "progress: 20 tensor(0.1866)\n",
            "\tgrad:  1.0 6.0 tensor(-1.2753) tensor(-1.2753)\n",
            "\t tensor(3.1141) tensor(2.2483)\n",
            "\tgrad:  2.0 16.0 tensor(-2.7070) tensor(-5.4140)\n",
            "\t tensor(3.1396) tensor(2.2610)\n",
            "\tgrad:  3.0 30.0 tensor(2.5057) tensor(7.5170)\n",
            "\t tensor(3.1937) tensor(2.3152)\n",
            "progress: 21 tensor(0.1744)\n",
            "\tgrad:  1.0 6.0 tensor(-1.2328) tensor(-1.2328)\n",
            "\t tensor(3.1436) tensor(2.2400)\n",
            "\tgrad:  2.0 16.0 tensor(-2.6168) tensor(-5.2336)\n",
            "\t tensor(3.1683) tensor(2.2523)\n",
            "\tgrad:  3.0 30.0 tensor(2.4221) tensor(7.2664)\n",
            "\t tensor(3.2206) tensor(2.3047)\n",
            "progress: 22 tensor(0.1630)\n",
            "\tgrad:  1.0 6.0 tensor(-1.1917) tensor(-1.1917)\n",
            "\t tensor(3.1722) tensor(2.2320)\n",
            "\tgrad:  2.0 16.0 tensor(-2.5295) tensor(-5.0591)\n",
            "\t tensor(3.1960) tensor(2.2439)\n",
            "\tgrad:  3.0 30.0 tensor(2.3414) tensor(7.0241)\n",
            "\t tensor(3.2466) tensor(2.2945)\n",
            "progress: 23 tensor(0.1523)\n",
            "\tgrad:  1.0 6.0 tensor(-1.1520) tensor(-1.1520)\n",
            "\t tensor(3.1998) tensor(2.2243)\n",
            "\tgrad:  2.0 16.0 tensor(-2.4452) tensor(-4.8904)\n",
            "\t tensor(3.2228) tensor(2.2358)\n",
            "\tgrad:  3.0 30.0 tensor(2.2633) tensor(6.7900)\n",
            "\t tensor(3.2717) tensor(2.2847)\n",
            "progress: 24 tensor(0.1423)\n",
            "\tgrad:  1.0 6.0 tensor(-1.1136) tensor(-1.1136)\n",
            "\t tensor(3.2264) tensor(2.2168)\n",
            "\tgrad:  2.0 16.0 tensor(-2.3637) tensor(-4.7274)\n",
            "\t tensor(3.2487) tensor(2.2279)\n",
            "\tgrad:  3.0 30.0 tensor(2.1879) tensor(6.5636)\n",
            "\t tensor(3.2960) tensor(2.2752)\n",
            "progress: 25 tensor(0.1330)\n",
            "\tgrad:  1.0 6.0 tensor(-1.0764) tensor(-1.0764)\n",
            "\t tensor(3.2522) tensor(2.2096)\n",
            "\tgrad:  2.0 16.0 tensor(-2.2849) tensor(-4.5698)\n",
            "\t tensor(3.2738) tensor(2.2203)\n",
            "\tgrad:  3.0 30.0 tensor(2.1149) tensor(6.3448)\n",
            "\t tensor(3.3195) tensor(2.2660)\n",
            "progress: 26 tensor(0.1243)\n",
            "\tgrad:  1.0 6.0 tensor(-1.0406) tensor(-1.0406)\n",
            "\t tensor(3.2772) tensor(2.2026)\n",
            "\tgrad:  2.0 16.0 tensor(-2.2087) tensor(-4.4175)\n",
            "\t tensor(3.2980) tensor(2.2130)\n",
            "\tgrad:  3.0 30.0 tensor(2.0444) tensor(6.1333)\n",
            "\t tensor(3.3421) tensor(2.2571)\n",
            "progress: 27 tensor(0.1161)\n",
            "\tgrad:  1.0 6.0 tensor(-1.0059) tensor(-1.0059)\n",
            "\t tensor(3.3012) tensor(2.1958)\n",
            "\tgrad:  2.0 16.0 tensor(-2.1351) tensor(-4.2702)\n",
            "\t tensor(3.3214) tensor(2.2059)\n",
            "\tgrad:  3.0 30.0 tensor(1.9763) tensor(5.9289)\n",
            "\t tensor(3.3641) tensor(2.2486)\n",
            "progress: 28 tensor(0.1085)\n",
            "\tgrad:  1.0 6.0 tensor(-0.9723) tensor(-0.9723)\n",
            "\t tensor(3.3245) tensor(2.1893)\n",
            "\tgrad:  2.0 16.0 tensor(-2.0639) tensor(-4.1279)\n",
            "\t tensor(3.3440) tensor(2.1990)\n",
            "\tgrad:  3.0 30.0 tensor(1.9104) tensor(5.7312)\n",
            "\t tensor(3.3853) tensor(2.2403)\n",
            "progress: 29 tensor(0.1014)\n",
            "\tgrad:  1.0 6.0 tensor(-0.9399) tensor(-0.9399)\n",
            "\t tensor(3.3471) tensor(2.1830)\n",
            "\tgrad:  2.0 16.0 tensor(-1.9951) tensor(-3.9902)\n",
            "\t tensor(3.3659) tensor(2.1924)\n",
            "\tgrad:  3.0 30.0 tensor(1.8467) tensor(5.5401)\n",
            "\t tensor(3.4058) tensor(2.2323)\n",
            "progress: 30 tensor(0.0947)\n",
            "\tgrad:  1.0 6.0 tensor(-0.9086) tensor(-0.9086)\n",
            "\t tensor(3.3688) tensor(2.1769)\n",
            "\tgrad:  2.0 16.0 tensor(-1.9286) tensor(-3.8572)\n",
            "\t tensor(3.3870) tensor(2.1860)\n",
            "\tgrad:  3.0 30.0 tensor(1.7852) tensor(5.3555)\n",
            "\t tensor(3.4256) tensor(2.2245)\n",
            "progress: 31 tensor(0.0885)\n",
            "\tgrad:  1.0 6.0 tensor(-0.8783) tensor(-0.8783)\n",
            "\t tensor(3.3899) tensor(2.1710)\n",
            "\tgrad:  2.0 16.0 tensor(-1.8643) tensor(-3.7286)\n",
            "\t tensor(3.4074) tensor(2.1798)\n",
            "\tgrad:  3.0 30.0 tensor(1.7256) tensor(5.1769)\n",
            "\t tensor(3.4447) tensor(2.2170)\n",
            "progress: 32 tensor(0.0827)\n",
            "\tgrad:  1.0 6.0 tensor(-0.8490) tensor(-0.8490)\n",
            "\t tensor(3.4102) tensor(2.1653)\n",
            "\tgrad:  2.0 16.0 tensor(-1.8022) tensor(-3.6043)\n",
            "\t tensor(3.4272) tensor(2.1738)\n",
            "\tgrad:  3.0 30.0 tensor(1.6681) tensor(5.0043)\n",
            "\t tensor(3.4632) tensor(2.2098)\n",
            "progress: 33 tensor(0.0773)\n",
            "\tgrad:  1.0 6.0 tensor(-0.8207) tensor(-0.8207)\n",
            "\t tensor(3.4299) tensor(2.1598)\n",
            "\tgrad:  2.0 16.0 tensor(-1.7421) tensor(-3.4842)\n",
            "\t tensor(3.4463) tensor(2.1680)\n",
            "\tgrad:  3.0 30.0 tensor(1.6125) tensor(4.8376)\n",
            "\t tensor(3.4811) tensor(2.2028)\n",
            "progress: 34 tensor(0.0722)\n",
            "\tgrad:  1.0 6.0 tensor(-0.7934) tensor(-0.7934)\n",
            "\t tensor(3.4489) tensor(2.1544)\n",
            "\tgrad:  2.0 16.0 tensor(-1.6840) tensor(-3.3680)\n",
            "\t tensor(3.4647) tensor(2.1624)\n",
            "\tgrad:  3.0 30.0 tensor(1.5588) tensor(4.6763)\n",
            "\t tensor(3.4984) tensor(2.1961)\n",
            "progress: 35 tensor(0.0675)\n",
            "\tgrad:  1.0 6.0 tensor(-0.7669) tensor(-0.7669)\n",
            "\t tensor(3.4672) tensor(2.1493)\n",
            "\tgrad:  2.0 16.0 tensor(-1.6279) tensor(-3.2558)\n",
            "\t tensor(3.4826) tensor(2.1570)\n",
            "\tgrad:  3.0 30.0 tensor(1.5068) tensor(4.5203)\n",
            "\t tensor(3.5151) tensor(2.1895)\n",
            "progress: 36 tensor(0.0631)\n",
            "\tgrad:  1.0 6.0 tensor(-0.7413) tensor(-0.7413)\n",
            "\t tensor(3.4850) tensor(2.1443)\n",
            "\tgrad:  2.0 16.0 tensor(-1.5736) tensor(-3.1472)\n",
            "\t tensor(3.4998) tensor(2.1517)\n",
            "\tgrad:  3.0 30.0 tensor(1.4566) tensor(4.3697)\n",
            "\t tensor(3.5313) tensor(2.1832)\n",
            "progress: 37 tensor(0.0589)\n",
            "\tgrad:  1.0 6.0 tensor(-0.7166) tensor(-0.7166)\n",
            "\t tensor(3.5022) tensor(2.1395)\n",
            "\tgrad:  2.0 16.0 tensor(-1.5212) tensor(-3.0423)\n",
            "\t tensor(3.5165) tensor(2.1467)\n",
            "\tgrad:  3.0 30.0 tensor(1.4080) tensor(4.2240)\n",
            "\t tensor(3.5469) tensor(2.1771)\n",
            "progress: 38 tensor(0.0551)\n",
            "\tgrad:  1.0 6.0 tensor(-0.6927) tensor(-0.6927)\n",
            "\t tensor(3.5188) tensor(2.1349)\n",
            "\tgrad:  2.0 16.0 tensor(-1.4704) tensor(-2.9409)\n",
            "\t tensor(3.5326) tensor(2.1418)\n",
            "\tgrad:  3.0 30.0 tensor(1.3610) tensor(4.0831)\n",
            "\t tensor(3.5620) tensor(2.1712)\n",
            "progress: 39 tensor(0.0515)\n",
            "\tgrad:  1.0 6.0 tensor(-0.6696) tensor(-0.6696)\n",
            "\t tensor(3.5348) tensor(2.1304)\n",
            "\tgrad:  2.0 16.0 tensor(-1.4214) tensor(-2.8428)\n",
            "\t tensor(3.5482) tensor(2.1371)\n",
            "\tgrad:  3.0 30.0 tensor(1.3157) tensor(3.9471)\n",
            "\t tensor(3.5766) tensor(2.1655)\n",
            "progress: 40 tensor(0.0481)\n",
            "\tgrad:  1.0 6.0 tensor(-0.6473) tensor(-0.6473)\n",
            "\t tensor(3.5503) tensor(2.1260)\n",
            "\tgrad:  2.0 16.0 tensor(-1.3740) tensor(-2.7481)\n",
            "\t tensor(3.5633) tensor(2.1325)\n",
            "\tgrad:  3.0 30.0 tensor(1.2718) tensor(3.8155)\n",
            "\t tensor(3.5907) tensor(2.1600)\n",
            "progress: 41 tensor(0.0449)\n",
            "\tgrad:  1.0 6.0 tensor(-0.6257) tensor(-0.6257)\n",
            "\t tensor(3.5653) tensor(2.1218)\n",
            "\tgrad:  2.0 16.0 tensor(-1.3282) tensor(-2.6565)\n",
            "\t tensor(3.5778) tensor(2.1281)\n",
            "\tgrad:  3.0 30.0 tensor(1.2294) tensor(3.6883)\n",
            "\t tensor(3.6044) tensor(2.1546)\n",
            "progress: 42 tensor(0.0420)\n",
            "\tgrad:  1.0 6.0 tensor(-0.6049) tensor(-0.6049)\n",
            "\t tensor(3.5798) tensor(2.1178)\n",
            "\tgrad:  2.0 16.0 tensor(-1.2840) tensor(-2.5679)\n",
            "\t tensor(3.5919) tensor(2.1238)\n",
            "\tgrad:  3.0 30.0 tensor(1.1885) tensor(3.5654)\n",
            "\t tensor(3.6176) tensor(2.1495)\n",
            "progress: 43 tensor(0.0392)\n",
            "\tgrad:  1.0 6.0 tensor(-0.5847) tensor(-0.5847)\n",
            "\t tensor(3.5938) tensor(2.1138)\n",
            "\tgrad:  2.0 16.0 tensor(-1.2412) tensor(-2.4823)\n",
            "\t tensor(3.6055) tensor(2.1197)\n",
            "\tgrad:  3.0 30.0 tensor(1.1488) tensor(3.4464)\n",
            "\t tensor(3.6303) tensor(2.1445)\n",
            "progress: 44 tensor(0.0367)\n",
            "\tgrad:  1.0 6.0 tensor(-0.5652) tensor(-0.5652)\n",
            "\t tensor(3.6074) tensor(2.1100)\n",
            "\tgrad:  2.0 16.0 tensor(-1.1998) tensor(-2.3995)\n",
            "\t tensor(3.6187) tensor(2.1157)\n",
            "\tgrad:  3.0 30.0 tensor(1.1105) tensor(3.3316)\n",
            "\t tensor(3.6427) tensor(2.1397)\n",
            "progress: 45 tensor(0.0343)\n",
            "\tgrad:  1.0 6.0 tensor(-0.5464) tensor(-0.5464)\n",
            "\t tensor(3.6204) tensor(2.1064)\n",
            "\tgrad:  2.0 16.0 tensor(-1.1598) tensor(-2.3195)\n",
            "\t tensor(3.6314) tensor(2.1118)\n",
            "\tgrad:  3.0 30.0 tensor(1.0735) tensor(3.2206)\n",
            "\t tensor(3.6546) tensor(2.1350)\n",
            "progress: 46 tensor(0.0320)\n",
            "\tgrad:  1.0 6.0 tensor(-0.5282) tensor(-0.5282)\n",
            "\t tensor(3.6331) tensor(2.1028)\n",
            "\tgrad:  2.0 16.0 tensor(-1.1211) tensor(-2.2422)\n",
            "\t tensor(3.6437) tensor(2.1081)\n",
            "\tgrad:  3.0 30.0 tensor(1.0377) tensor(3.1131)\n",
            "\t tensor(3.6661) tensor(2.1305)\n",
            "progress: 47 tensor(0.0299)\n",
            "\tgrad:  1.0 6.0 tensor(-0.5106) tensor(-0.5106)\n",
            "\t tensor(3.6453) tensor(2.0994)\n",
            "\tgrad:  2.0 16.0 tensor(-1.0837) tensor(-2.1675)\n",
            "\t tensor(3.6555) tensor(2.1045)\n",
            "\tgrad:  3.0 30.0 tensor(1.0031) tensor(3.0094)\n",
            "\t tensor(3.6772) tensor(2.1262)\n",
            "progress: 48 tensor(0.0280)\n",
            "\tgrad:  1.0 6.0 tensor(-0.4935) tensor(-0.4935)\n",
            "\t tensor(3.6571) tensor(2.0961)\n",
            "\tgrad:  2.0 16.0 tensor(-1.0476) tensor(-2.0952)\n",
            "\t tensor(3.6670) tensor(2.1010)\n",
            "\tgrad:  3.0 30.0 tensor(0.9697) tensor(2.9090)\n",
            "\t tensor(3.6880) tensor(2.1220)\n",
            "progress: 49 tensor(0.0261)\n",
            "\tgrad:  1.0 6.0 tensor(-0.4771) tensor(-0.4771)\n",
            "\t tensor(3.6686) tensor(2.0929)\n",
            "\tgrad:  2.0 16.0 tensor(-1.0127) tensor(-2.0254)\n",
            "\t tensor(3.6781) tensor(2.0976)\n",
            "\tgrad:  3.0 30.0 tensor(0.9374) tensor(2.8121)\n",
            "\t tensor(3.6984) tensor(2.1179)\n",
            "progress: 50 tensor(0.0244)\n",
            "\tgrad:  1.0 6.0 tensor(-0.4612) tensor(-0.4612)\n",
            "\t tensor(3.6796) tensor(2.0898)\n",
            "\tgrad:  2.0 16.0 tensor(-0.9789) tensor(-1.9579)\n",
            "\t tensor(3.6889) tensor(2.0944)\n",
            "\tgrad:  3.0 30.0 tensor(0.9061) tensor(2.7183)\n",
            "\t tensor(3.7084) tensor(2.1140)\n",
            "progress: 51 tensor(0.0228)\n",
            "\tgrad:  1.0 6.0 tensor(-0.4458) tensor(-0.4458)\n",
            "\t tensor(3.6903) tensor(2.0868)\n",
            "\tgrad:  2.0 16.0 tensor(-0.9463) tensor(-1.8926)\n",
            "\t tensor(3.6992) tensor(2.0912)\n",
            "\tgrad:  3.0 30.0 tensor(0.8759) tensor(2.6277)\n",
            "\t tensor(3.7181) tensor(2.1102)\n",
            "progress: 52 tensor(0.0213)\n",
            "\tgrad:  1.0 6.0 tensor(-0.4310) tensor(-0.4310)\n",
            "\t tensor(3.7006) tensor(2.0839)\n",
            "\tgrad:  2.0 16.0 tensor(-0.9147) tensor(-1.8295)\n",
            "\t tensor(3.7092) tensor(2.0882)\n",
            "\tgrad:  3.0 30.0 tensor(0.8467) tensor(2.5402)\n",
            "\t tensor(3.7275) tensor(2.1065)\n",
            "progress: 53 tensor(0.0199)\n",
            "\tgrad:  1.0 6.0 tensor(-0.4166) tensor(-0.4166)\n",
            "\t tensor(3.7106) tensor(2.0811)\n",
            "\tgrad:  2.0 16.0 tensor(-0.8843) tensor(-1.7685)\n",
            "\t tensor(3.7189) tensor(2.0853)\n",
            "\tgrad:  3.0 30.0 tensor(0.8185) tensor(2.4554)\n",
            "\t tensor(3.7366) tensor(2.1029)\n",
            "progress: 54 tensor(0.0186)\n",
            "\tgrad:  1.0 6.0 tensor(-0.4027) tensor(-0.4027)\n",
            "\t tensor(3.7203) tensor(2.0784)\n",
            "\tgrad:  2.0 16.0 tensor(-0.8548) tensor(-1.7095)\n",
            "\t tensor(3.7283) tensor(2.0824)\n",
            "\tgrad:  3.0 30.0 tensor(0.7912) tensor(2.3736)\n",
            "\t tensor(3.7454) tensor(2.0995)\n",
            "progress: 55 tensor(0.0174)\n",
            "\tgrad:  1.0 6.0 tensor(-0.3893) tensor(-0.3893)\n",
            "\t tensor(3.7296) tensor(2.0758)\n",
            "\tgrad:  2.0 16.0 tensor(-0.8263) tensor(-1.6526)\n",
            "\t tensor(3.7374) tensor(2.0797)\n",
            "\tgrad:  3.0 30.0 tensor(0.7648) tensor(2.2945)\n",
            "\t tensor(3.7539) tensor(2.0962)\n",
            "progress: 56 tensor(0.0162)\n",
            "\tgrad:  1.0 6.0 tensor(-0.3763) tensor(-0.3763)\n",
            "\t tensor(3.7386) tensor(2.0733)\n",
            "\tgrad:  2.0 16.0 tensor(-0.7987) tensor(-1.5975)\n",
            "\t tensor(3.7461) tensor(2.0770)\n",
            "\tgrad:  3.0 30.0 tensor(0.7393) tensor(2.2179)\n",
            "\t tensor(3.7621) tensor(2.0930)\n",
            "progress: 57 tensor(0.0152)\n",
            "\tgrad:  1.0 6.0 tensor(-0.3638) tensor(-0.3638)\n",
            "\t tensor(3.7473) tensor(2.0708)\n",
            "\tgrad:  2.0 16.0 tensor(-0.7721) tensor(-1.5442)\n",
            "\t tensor(3.7546) tensor(2.0744)\n",
            "\tgrad:  3.0 30.0 tensor(0.7147) tensor(2.1441)\n",
            "\t tensor(3.7700) tensor(2.0899)\n",
            "progress: 58 tensor(0.0142)\n",
            "\tgrad:  1.0 6.0 tensor(-0.3516) tensor(-0.3516)\n",
            "\t tensor(3.7557) tensor(2.0685)\n",
            "\tgrad:  2.0 16.0 tensor(-0.7464) tensor(-1.4928)\n",
            "\t tensor(3.7628) tensor(2.0720)\n",
            "\tgrad:  3.0 30.0 tensor(0.6909) tensor(2.0726)\n",
            "\t tensor(3.7777) tensor(2.0869)\n",
            "progress: 59 tensor(0.0133)\n",
            "\tgrad:  1.0 6.0 tensor(-0.3399) tensor(-0.3399)\n",
            "\t tensor(3.7639) tensor(2.0662)\n",
            "\tgrad:  2.0 16.0 tensor(-0.7215) tensor(-1.4430)\n",
            "\t tensor(3.7707) tensor(2.0696)\n",
            "\tgrad:  3.0 30.0 tensor(0.6678) tensor(2.0035)\n",
            "\t tensor(3.7851) tensor(2.0840)\n",
            "progress: 60 tensor(0.0124)\n",
            "\tgrad:  1.0 6.0 tensor(-0.3286) tensor(-0.3286)\n",
            "\t tensor(3.7718) tensor(2.0640)\n",
            "\tgrad:  2.0 16.0 tensor(-0.6974) tensor(-1.3949)\n",
            "\t tensor(3.7783) tensor(2.0672)\n",
            "\tgrad:  3.0 30.0 tensor(0.6456) tensor(1.9367)\n",
            "\t tensor(3.7923) tensor(2.0812)\n",
            "progress: 61 tensor(0.0116)\n",
            "\tgrad:  1.0 6.0 tensor(-0.3176) tensor(-0.3176)\n",
            "\t tensor(3.7794) tensor(2.0618)\n",
            "\tgrad:  2.0 16.0 tensor(-0.6742) tensor(-1.3484)\n",
            "\t tensor(3.7857) tensor(2.0650)\n",
            "\tgrad:  3.0 30.0 tensor(0.6240) tensor(1.8721)\n",
            "\t tensor(3.7992) tensor(2.0785)\n",
            "progress: 62 tensor(0.0108)\n",
            "\tgrad:  1.0 6.0 tensor(-0.3070) tensor(-0.3070)\n",
            "\t tensor(3.7867) tensor(2.0598)\n",
            "\tgrad:  2.0 16.0 tensor(-0.6517) tensor(-1.3034)\n",
            "\t tensor(3.7929) tensor(2.0628)\n",
            "\tgrad:  3.0 30.0 tensor(0.6032) tensor(1.8097)\n",
            "\t tensor(3.8059) tensor(2.0759)\n",
            "progress: 63 tensor(0.0101)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2968) tensor(-0.2968)\n",
            "\t tensor(3.7938) tensor(2.0578)\n",
            "\tgrad:  2.0 16.0 tensor(-0.6300) tensor(-1.2600)\n",
            "\t tensor(3.7998) tensor(2.0607)\n",
            "\tgrad:  3.0 30.0 tensor(0.5831) tensor(1.7494)\n",
            "\t tensor(3.8124) tensor(2.0733)\n",
            "progress: 64 tensor(0.0094)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2869) tensor(-0.2869)\n",
            "\t tensor(3.8007) tensor(2.0559)\n",
            "\tgrad:  2.0 16.0 tensor(-0.6090) tensor(-1.2180)\n",
            "\t tensor(3.8064) tensor(2.0587)\n",
            "\tgrad:  3.0 30.0 tensor(0.5637) tensor(1.6911)\n",
            "\t tensor(3.8186) tensor(2.0709)\n",
            "progress: 65 tensor(0.0088)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2773) tensor(-0.2773)\n",
            "\t tensor(3.8073) tensor(2.0540)\n",
            "\tgrad:  2.0 16.0 tensor(-0.5887) tensor(-1.1774)\n",
            "\t tensor(3.8129) tensor(2.0568)\n",
            "\tgrad:  3.0 30.0 tensor(0.5449) tensor(1.6347)\n",
            "\t tensor(3.8247) tensor(2.0685)\n",
            "progress: 66 tensor(0.0082)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2681) tensor(-0.2681)\n",
            "\t tensor(3.8138) tensor(2.0522)\n",
            "\tgrad:  2.0 16.0 tensor(-0.5691) tensor(-1.1381)\n",
            "\t tensor(3.8191) tensor(2.0549)\n",
            "\tgrad:  3.0 30.0 tensor(0.5267) tensor(1.5802)\n",
            "\t tensor(3.8305) tensor(2.0663)\n",
            "progress: 67 tensor(0.0077)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2592) tensor(-0.2592)\n",
            "\t tensor(3.8200) tensor(2.0504)\n",
            "\tgrad:  2.0 16.0 tensor(-0.5501) tensor(-1.1002)\n",
            "\t tensor(3.8252) tensor(2.0530)\n",
            "\tgrad:  3.0 30.0 tensor(0.5092) tensor(1.5275)\n",
            "\t tensor(3.8362) tensor(2.0640)\n",
            "progress: 68 tensor(0.0072)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2505) tensor(-0.2505)\n",
            "\t tensor(3.8260) tensor(2.0488)\n",
            "\tgrad:  2.0 16.0 tensor(-0.5317) tensor(-1.0635)\n",
            "\t tensor(3.8310) tensor(2.0513)\n",
            "\tgrad:  3.0 30.0 tensor(0.4922) tensor(1.4766)\n",
            "\t tensor(3.8416) tensor(2.0619)\n",
            "progress: 69 tensor(0.0067)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2422) tensor(-0.2422)\n",
            "\t tensor(3.8318) tensor(2.0471)\n",
            "\tgrad:  2.0 16.0 tensor(-0.5140) tensor(-1.0281)\n",
            "\t tensor(3.8366) tensor(2.0496)\n",
            "\tgrad:  3.0 30.0 tensor(0.4758) tensor(1.4274)\n",
            "\t tensor(3.8469) tensor(2.0598)\n",
            "progress: 70 tensor(0.0063)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2341) tensor(-0.2341)\n",
            "\t tensor(3.8374) tensor(2.0456)\n",
            "\tgrad:  2.0 16.0 tensor(-0.4969) tensor(-0.9938)\n",
            "\t tensor(3.8421) tensor(2.0479)\n",
            "\tgrad:  3.0 30.0 tensor(0.4599) tensor(1.3798)\n",
            "\t tensor(3.8520) tensor(2.0578)\n",
            "progress: 71 tensor(0.0059)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2263) tensor(-0.2263)\n",
            "\t tensor(3.8428) tensor(2.0441)\n",
            "\tgrad:  2.0 16.0 tensor(-0.4803) tensor(-0.9606)\n",
            "\t tensor(3.8473) tensor(2.0463)\n",
            "\tgrad:  3.0 30.0 tensor(0.4446) tensor(1.3337)\n",
            "\t tensor(3.8569) tensor(2.0559)\n",
            "progress: 72 tensor(0.0055)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2187) tensor(-0.2187)\n",
            "\t tensor(3.8480) tensor(2.0426)\n",
            "\tgrad:  2.0 16.0 tensor(-0.4643) tensor(-0.9286)\n",
            "\t tensor(3.8524) tensor(2.0448)\n",
            "\tgrad:  3.0 30.0 tensor(0.4298) tensor(1.2893)\n",
            "\t tensor(3.8617) tensor(2.0541)\n",
            "progress: 73 tensor(0.0051)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2115) tensor(-0.2115)\n",
            "\t tensor(3.8531) tensor(2.0412)\n",
            "\tgrad:  2.0 16.0 tensor(-0.4488) tensor(-0.8977)\n",
            "\t tensor(3.8573) tensor(2.0433)\n",
            "\tgrad:  3.0 30.0 tensor(0.4154) tensor(1.2463)\n",
            "\t tensor(3.8663) tensor(2.0523)\n",
            "progress: 74 tensor(0.0048)\n",
            "\tgrad:  1.0 6.0 tensor(-0.2044) tensor(-0.2044)\n",
            "\t tensor(3.8580) tensor(2.0398)\n",
            "\tgrad:  2.0 16.0 tensor(-0.4339) tensor(-0.8677)\n",
            "\t tensor(3.8621) tensor(2.0418)\n",
            "\tgrad:  3.0 30.0 tensor(0.4016) tensor(1.2048)\n",
            "\t tensor(3.8708) tensor(2.0505)\n",
            "progress: 75 tensor(0.0045)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1976) tensor(-0.1976)\n",
            "\t tensor(3.8627) tensor(2.0385)\n",
            "\tgrad:  2.0 16.0 tensor(-0.4194) tensor(-0.8388)\n",
            "\t tensor(3.8667) tensor(2.0404)\n",
            "\tgrad:  3.0 30.0 tensor(0.3882) tensor(1.1646)\n",
            "\t tensor(3.8751) tensor(2.0488)\n",
            "progress: 76 tensor(0.0042)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1910) tensor(-0.1910)\n",
            "\t tensor(3.8673) tensor(2.0372)\n",
            "\tgrad:  2.0 16.0 tensor(-0.4054) tensor(-0.8109)\n",
            "\t tensor(3.8711) tensor(2.0391)\n",
            "\tgrad:  3.0 30.0 tensor(0.3753) tensor(1.1258)\n",
            "\t tensor(3.8792) tensor(2.0472)\n",
            "progress: 77 tensor(0.0039)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1846) tensor(-0.1846)\n",
            "\t tensor(3.8717) tensor(2.0359)\n",
            "\tgrad:  2.0 16.0 tensor(-0.3919) tensor(-0.7838)\n",
            "\t tensor(3.8754) tensor(2.0378)\n",
            "\tgrad:  3.0 30.0 tensor(0.3628) tensor(1.0883)\n",
            "\t tensor(3.8833) tensor(2.0456)\n",
            "progress: 78 tensor(0.0037)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1785) tensor(-0.1785)\n",
            "\t tensor(3.8760) tensor(2.0347)\n",
            "\tgrad:  2.0 16.0 tensor(-0.3788) tensor(-0.7577)\n",
            "\t tensor(3.8796) tensor(2.0365)\n",
            "\tgrad:  3.0 30.0 tensor(0.3506) tensor(1.0519)\n",
            "\t tensor(3.8872) tensor(2.0441)\n",
            "progress: 79 tensor(0.0034)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1725) tensor(-0.1725)\n",
            "\t tensor(3.8801) tensor(2.0336)\n",
            "\tgrad:  2.0 16.0 tensor(-0.3662) tensor(-0.7324)\n",
            "\t tensor(3.8836) tensor(2.0353)\n",
            "\tgrad:  3.0 30.0 tensor(0.3390) tensor(1.0170)\n",
            "\t tensor(3.8909) tensor(2.0426)\n",
            "progress: 80 tensor(0.0032)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1668) tensor(-0.1668)\n",
            "\t tensor(3.8841) tensor(2.0325)\n",
            "\tgrad:  2.0 16.0 tensor(-0.3540) tensor(-0.7080)\n",
            "\t tensor(3.8875) tensor(2.0341)\n",
            "\tgrad:  3.0 30.0 tensor(0.3276) tensor(0.9829)\n",
            "\t tensor(3.8946) tensor(2.0412)\n",
            "progress: 81 tensor(0.0030)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1612) tensor(-0.1612)\n",
            "\t tensor(3.8880) tensor(2.0314)\n",
            "\tgrad:  2.0 16.0 tensor(-0.3422) tensor(-0.6844)\n",
            "\t tensor(3.8912) tensor(2.0330)\n",
            "\tgrad:  3.0 30.0 tensor(0.3168) tensor(0.9503)\n",
            "\t tensor(3.8981) tensor(2.0398)\n",
            "progress: 82 tensor(0.0028)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1558) tensor(-0.1558)\n",
            "\t tensor(3.8917) tensor(2.0303)\n",
            "\tgrad:  2.0 16.0 tensor(-0.3308) tensor(-0.6616)\n",
            "\t tensor(3.8949) tensor(2.0319)\n",
            "\tgrad:  3.0 30.0 tensor(0.3062) tensor(0.9186)\n",
            "\t tensor(3.9015) tensor(2.0385)\n",
            "progress: 83 tensor(0.0026)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1506) tensor(-0.1506)\n",
            "\t tensor(3.8953) tensor(2.0293)\n",
            "\tgrad:  2.0 16.0 tensor(-0.3198) tensor(-0.6395)\n",
            "\t tensor(3.8984) tensor(2.0308)\n",
            "\tgrad:  3.0 30.0 tensor(0.2960) tensor(0.8880)\n",
            "\t tensor(3.9048) tensor(2.0372)\n",
            "progress: 84 tensor(0.0024)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1456) tensor(-0.1456)\n",
            "\t tensor(3.8988) tensor(2.0283)\n",
            "\tgrad:  2.0 16.0 tensor(-0.3091) tensor(-0.6182)\n",
            "\t tensor(3.9018) tensor(2.0298)\n",
            "\tgrad:  3.0 30.0 tensor(0.2861) tensor(0.8583)\n",
            "\t tensor(3.9079) tensor(2.0360)\n",
            "progress: 85 tensor(0.0023)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1408) tensor(-0.1408)\n",
            "\t tensor(3.9022) tensor(2.0274)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2988) tensor(-0.5976)\n",
            "\t tensor(3.9050) tensor(2.0288)\n",
            "\tgrad:  3.0 30.0 tensor(0.2766) tensor(0.8298)\n",
            "\t tensor(3.9110) tensor(2.0348)\n",
            "progress: 86 tensor(0.0021)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1361) tensor(-0.1361)\n",
            "\t tensor(3.9055) tensor(2.0265)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2888) tensor(-0.5777)\n",
            "\t tensor(3.9082) tensor(2.0279)\n",
            "\tgrad:  3.0 30.0 tensor(0.2674) tensor(0.8021)\n",
            "\t tensor(3.9140) tensor(2.0336)\n",
            "progress: 87 tensor(0.0020)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1315) tensor(-0.1315)\n",
            "\t tensor(3.9086) tensor(2.0256)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2792) tensor(-0.5584)\n",
            "\t tensor(3.9113) tensor(2.0269)\n",
            "\tgrad:  3.0 30.0 tensor(0.2584) tensor(0.7753)\n",
            "\t tensor(3.9168) tensor(2.0325)\n",
            "progress: 88 tensor(0.0019)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1272) tensor(-0.1272)\n",
            "\t tensor(3.9117) tensor(2.0248)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2699) tensor(-0.5398)\n",
            "\t tensor(3.9142) tensor(2.0260)\n",
            "\tgrad:  3.0 30.0 tensor(0.2498) tensor(0.7495)\n",
            "\t tensor(3.9196) tensor(2.0314)\n",
            "progress: 89 tensor(0.0017)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1229) tensor(-0.1229)\n",
            "\t tensor(3.9146) tensor(2.0239)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2609) tensor(-0.5218)\n",
            "\t tensor(3.9171) tensor(2.0252)\n",
            "\tgrad:  3.0 30.0 tensor(0.2415) tensor(0.7245)\n",
            "\t tensor(3.9223) tensor(2.0304)\n",
            "progress: 90 tensor(0.0016)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1188) tensor(-0.1188)\n",
            "\t tensor(3.9175) tensor(2.0231)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2522) tensor(-0.5044)\n",
            "\t tensor(3.9198) tensor(2.0243)\n",
            "\tgrad:  3.0 30.0 tensor(0.2335) tensor(0.7004)\n",
            "\t tensor(3.9249) tensor(2.0294)\n",
            "progress: 91 tensor(0.0015)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1149) tensor(-0.1149)\n",
            "\t tensor(3.9202) tensor(2.0224)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2438) tensor(-0.4876)\n",
            "\t tensor(3.9225) tensor(2.0235)\n",
            "\tgrad:  3.0 30.0 tensor(0.2257) tensor(0.6770)\n",
            "\t tensor(3.9274) tensor(2.0284)\n",
            "progress: 92 tensor(0.0014)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1110) tensor(-0.1110)\n",
            "\t tensor(3.9229) tensor(2.0216)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2357) tensor(-0.4714)\n",
            "\t tensor(3.9251) tensor(2.0227)\n",
            "\tgrad:  3.0 30.0 tensor(0.2182) tensor(0.6545)\n",
            "\t tensor(3.9298) tensor(2.0274)\n",
            "progress: 93 tensor(0.0013)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1073) tensor(-0.1073)\n",
            "\t tensor(3.9254) tensor(2.0209)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2278) tensor(-0.4557)\n",
            "\t tensor(3.9276) tensor(2.0220)\n",
            "\tgrad:  3.0 30.0 tensor(0.2109) tensor(0.6326)\n",
            "\t tensor(3.9321) tensor(2.0265)\n",
            "progress: 94 tensor(0.0012)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1038) tensor(-0.1038)\n",
            "\t tensor(3.9279) tensor(2.0202)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2202) tensor(-0.4404)\n",
            "\t tensor(3.9300) tensor(2.0212)\n",
            "\tgrad:  3.0 30.0 tensor(0.2038) tensor(0.6115)\n",
            "\t tensor(3.9344) tensor(2.0256)\n",
            "progress: 95 tensor(0.0012)\n",
            "\tgrad:  1.0 6.0 tensor(-0.1003) tensor(-0.1003)\n",
            "\t tensor(3.9303) tensor(2.0195)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2129) tensor(-0.4258)\n",
            "\t tensor(3.9323) tensor(2.0205)\n",
            "\tgrad:  3.0 30.0 tensor(0.1971) tensor(0.5912)\n",
            "\t tensor(3.9366) tensor(2.0248)\n",
            "progress: 96 tensor(0.0011)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0970) tensor(-0.0970)\n",
            "\t tensor(3.9327) tensor(2.0189)\n",
            "\tgrad:  2.0 16.0 tensor(-0.2058) tensor(-0.4116)\n",
            "\t tensor(3.9346) tensor(2.0198)\n",
            "\tgrad:  3.0 30.0 tensor(0.1905) tensor(0.5714)\n",
            "\t tensor(3.9387) tensor(2.0240)\n",
            "progress: 97 tensor(0.0010)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0937) tensor(-0.0937)\n",
            "\t tensor(3.9349) tensor(2.0182)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1989) tensor(-0.3978)\n",
            "\t tensor(3.9368) tensor(2.0192)\n",
            "\tgrad:  3.0 30.0 tensor(0.1841) tensor(0.5524)\n",
            "\t tensor(3.9408) tensor(2.0232)\n",
            "progress: 98 tensor(0.0009)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0906) tensor(-0.0906)\n",
            "\t tensor(3.9371) tensor(2.0176)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1923) tensor(-0.3846)\n",
            "\t tensor(3.9389) tensor(2.0185)\n",
            "\tgrad:  3.0 30.0 tensor(0.1780) tensor(0.5340)\n",
            "\t tensor(3.9427) tensor(2.0224)\n",
            "progress: 99 tensor(0.0009)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0876) tensor(-0.0876)\n",
            "\t tensor(3.9392) tensor(2.0170)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1859) tensor(-0.3718)\n",
            "\t tensor(3.9409) tensor(2.0179)\n",
            "\tgrad:  3.0 30.0 tensor(0.1721) tensor(0.5162)\n",
            "\t tensor(3.9446) tensor(2.0216)\n",
            "progress: 100 tensor(0.0008)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0847) tensor(-0.0847)\n",
            "\t tensor(3.9412) tensor(2.0165)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1797) tensor(-0.3594)\n",
            "\t tensor(3.9429) tensor(2.0173)\n",
            "\tgrad:  3.0 30.0 tensor(0.1663) tensor(0.4990)\n",
            "\t tensor(3.9465) tensor(2.0209)\n",
            "progress: 101 tensor(0.0008)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0818) tensor(-0.0818)\n",
            "\t tensor(3.9432) tensor(2.0159)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1737) tensor(-0.3474)\n",
            "\t tensor(3.9448) tensor(2.0167)\n",
            "\tgrad:  3.0 30.0 tensor(0.1608) tensor(0.4823)\n",
            "\t tensor(3.9483) tensor(2.0202)\n",
            "progress: 102 tensor(0.0007)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0791) tensor(-0.0791)\n",
            "\t tensor(3.9450) tensor(2.0154)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1679) tensor(-0.3358)\n",
            "\t tensor(3.9466) tensor(2.0162)\n",
            "\tgrad:  3.0 30.0 tensor(0.1554) tensor(0.4663)\n",
            "\t tensor(3.9500) tensor(2.0195)\n",
            "progress: 103 tensor(0.0007)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0765) tensor(-0.0765)\n",
            "\t tensor(3.9469) tensor(2.0149)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1623) tensor(-0.3246)\n",
            "\t tensor(3.9484) tensor(2.0157)\n",
            "\tgrad:  3.0 30.0 tensor(0.1502) tensor(0.4507)\n",
            "\t tensor(3.9517) tensor(2.0189)\n",
            "progress: 104 tensor(0.0006)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0739) tensor(-0.0739)\n",
            "\t tensor(3.9487) tensor(2.0144)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1569) tensor(-0.3138)\n",
            "\t tensor(3.9501) tensor(2.0151)\n",
            "\tgrad:  3.0 30.0 tensor(0.1452) tensor(0.4357)\n",
            "\t tensor(3.9533) tensor(2.0183)\n",
            "progress: 105 tensor(0.0006)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0715) tensor(-0.0715)\n",
            "\t tensor(3.9504) tensor(2.0139)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1517) tensor(-0.3033)\n",
            "\t tensor(3.9518) tensor(2.0146)\n",
            "\tgrad:  3.0 30.0 tensor(0.1404) tensor(0.4212)\n",
            "\t tensor(3.9548) tensor(2.0177)\n",
            "progress: 106 tensor(0.0005)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0691) tensor(-0.0691)\n",
            "\t tensor(3.9520) tensor(2.0134)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1466) tensor(-0.2932)\n",
            "\t tensor(3.9534) tensor(2.0141)\n",
            "\tgrad:  3.0 30.0 tensor(0.1357) tensor(0.4071)\n",
            "\t tensor(3.9563) tensor(2.0171)\n",
            "progress: 107 tensor(0.0005)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0668) tensor(-0.0668)\n",
            "\t tensor(3.9536) tensor(2.0130)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1417) tensor(-0.2835)\n",
            "\t tensor(3.9550) tensor(2.0137)\n",
            "\tgrad:  3.0 30.0 tensor(0.1312) tensor(0.3935)\n",
            "\t tensor(3.9578) tensor(2.0165)\n",
            "progress: 108 tensor(0.0005)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0645) tensor(-0.0645)\n",
            "\t tensor(3.9552) tensor(2.0126)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1370) tensor(-0.2740)\n",
            "\t tensor(3.9565) tensor(2.0132)\n",
            "\tgrad:  3.0 30.0 tensor(0.1268) tensor(0.3804)\n",
            "\t tensor(3.9592) tensor(2.0159)\n",
            "progress: 109 tensor(0.0004)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0624) tensor(-0.0624)\n",
            "\t tensor(3.9567) tensor(2.0121)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1324) tensor(-0.2648)\n",
            "\t tensor(3.9579) tensor(2.0128)\n",
            "\tgrad:  3.0 30.0 tensor(0.1226) tensor(0.3678)\n",
            "\t tensor(3.9606) tensor(2.0154)\n",
            "progress: 110 tensor(0.0004)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0603) tensor(-0.0603)\n",
            "\t tensor(3.9581) tensor(2.0117)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1280) tensor(-0.2560)\n",
            "\t tensor(3.9593) tensor(2.0123)\n",
            "\tgrad:  3.0 30.0 tensor(0.1185) tensor(0.3555)\n",
            "\t tensor(3.9619) tensor(2.0149)\n",
            "progress: 111 tensor(0.0004)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0583) tensor(-0.0583)\n",
            "\t tensor(3.9595) tensor(2.0113)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1238) tensor(-0.2475)\n",
            "\t tensor(3.9607) tensor(2.0119)\n",
            "\tgrad:  3.0 30.0 tensor(0.1145) tensor(0.3436)\n",
            "\t tensor(3.9631) tensor(2.0144)\n",
            "progress: 112 tensor(0.0004)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0564) tensor(-0.0564)\n",
            "\t tensor(3.9609) tensor(2.0110)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1196) tensor(-0.2393)\n",
            "\t tensor(3.9620) tensor(2.0115)\n",
            "\tgrad:  3.0 30.0 tensor(0.1107) tensor(0.3322)\n",
            "\t tensor(3.9644) tensor(2.0139)\n",
            "progress: 113 tensor(0.0003)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0545) tensor(-0.0545)\n",
            "\t tensor(3.9622) tensor(2.0106)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1156) tensor(-0.2313)\n",
            "\t tensor(3.9632) tensor(2.0112)\n",
            "\tgrad:  3.0 30.0 tensor(0.1070) tensor(0.3211)\n",
            "\t tensor(3.9656) tensor(2.0135)\n",
            "progress: 114 tensor(0.0003)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0527) tensor(-0.0527)\n",
            "\t tensor(3.9634) tensor(2.0103)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1118) tensor(-0.2236)\n",
            "\t tensor(3.9645) tensor(2.0108)\n",
            "\tgrad:  3.0 30.0 tensor(0.1035) tensor(0.3104)\n",
            "\t tensor(3.9667) tensor(2.0130)\n",
            "progress: 115 tensor(0.0003)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0509) tensor(-0.0509)\n",
            "\t tensor(3.9646) tensor(2.0099)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1081) tensor(-0.2161)\n",
            "\t tensor(3.9657) tensor(2.0104)\n",
            "\tgrad:  3.0 30.0 tensor(0.1000) tensor(0.3001)\n",
            "\t tensor(3.9678) tensor(2.0126)\n",
            "progress: 116 tensor(0.0003)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0492) tensor(-0.0492)\n",
            "\t tensor(3.9658) tensor(2.0096)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1045) tensor(-0.2089)\n",
            "\t tensor(3.9668) tensor(2.0101)\n",
            "\tgrad:  3.0 30.0 tensor(0.0967) tensor(0.2900)\n",
            "\t tensor(3.9689) tensor(2.0122)\n",
            "progress: 117 tensor(0.0003)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0476) tensor(-0.0476)\n",
            "\t tensor(3.9670) tensor(2.0093)\n",
            "\tgrad:  2.0 16.0 tensor(-0.1010) tensor(-0.2019)\n",
            "\t tensor(3.9679) tensor(2.0097)\n",
            "\tgrad:  3.0 30.0 tensor(0.0935) tensor(0.2804)\n",
            "\t tensor(3.9699) tensor(2.0118)\n",
            "progress: 118 tensor(0.0002)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0460) tensor(-0.0460)\n",
            "\t tensor(3.9681) tensor(2.0090)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0976) tensor(-0.1952)\n",
            "\t tensor(3.9690) tensor(2.0094)\n",
            "\tgrad:  3.0 30.0 tensor(0.0904) tensor(0.2711)\n",
            "\t tensor(3.9709) tensor(2.0114)\n",
            "progress: 119 tensor(0.0002)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0445) tensor(-0.0445)\n",
            "\t tensor(3.9691) tensor(2.0087)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0944) tensor(-0.1887)\n",
            "\t tensor(3.9700) tensor(2.0091)\n",
            "\tgrad:  3.0 30.0 tensor(0.0873) tensor(0.2620)\n",
            "\t tensor(3.9719) tensor(2.0110)\n",
            "progress: 120 tensor(0.0002)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0430) tensor(-0.0430)\n",
            "\t tensor(3.9702) tensor(2.0084)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0912) tensor(-0.1824)\n",
            "\t tensor(3.9710) tensor(2.0088)\n",
            "\tgrad:  3.0 30.0 tensor(0.0844) tensor(0.2532)\n",
            "\t tensor(3.9728) tensor(2.0106)\n",
            "progress: 121 tensor(0.0002)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0415) tensor(-0.0415)\n",
            "\t tensor(3.9711) tensor(2.0081)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0882) tensor(-0.1763)\n",
            "\t tensor(3.9720) tensor(2.0085)\n",
            "\tgrad:  3.0 30.0 tensor(0.0816) tensor(0.2449)\n",
            "\t tensor(3.9737) tensor(2.0103)\n",
            "progress: 122 tensor(0.0002)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0402) tensor(-0.0402)\n",
            "\t tensor(3.9721) tensor(2.0078)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0852) tensor(-0.1705)\n",
            "\t tensor(3.9729) tensor(2.0082)\n",
            "\tgrad:  3.0 30.0 tensor(0.0789) tensor(0.2366)\n",
            "\t tensor(3.9746) tensor(2.0099)\n",
            "progress: 123 tensor(0.0002)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0388) tensor(-0.0388)\n",
            "\t tensor(3.9730) tensor(2.0076)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0824) tensor(-0.1647)\n",
            "\t tensor(3.9738) tensor(2.0079)\n",
            "\tgrad:  3.0 30.0 tensor(0.0763) tensor(0.2288)\n",
            "\t tensor(3.9755) tensor(2.0096)\n",
            "progress: 124 tensor(0.0002)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0375) tensor(-0.0375)\n",
            "\t tensor(3.9739) tensor(2.0073)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0796) tensor(-0.1593)\n",
            "\t tensor(3.9747) tensor(2.0077)\n",
            "\tgrad:  3.0 30.0 tensor(0.0737) tensor(0.2211)\n",
            "\t tensor(3.9763) tensor(2.0093)\n",
            "progress: 125 tensor(0.0002)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0363) tensor(-0.0363)\n",
            "\t tensor(3.9748) tensor(2.0071)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0770) tensor(-0.1540)\n",
            "\t tensor(3.9755) tensor(2.0074)\n",
            "\tgrad:  3.0 30.0 tensor(0.0713) tensor(0.2138)\n",
            "\t tensor(3.9771) tensor(2.0090)\n",
            "progress: 126 tensor(0.0001)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0351) tensor(-0.0351)\n",
            "\t tensor(3.9756) tensor(2.0068)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0744) tensor(-0.1488)\n",
            "\t tensor(3.9763) tensor(2.0072)\n",
            "\tgrad:  3.0 30.0 tensor(0.0689) tensor(0.2066)\n",
            "\t tensor(3.9778) tensor(2.0087)\n",
            "progress: 127 tensor(0.0001)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0339) tensor(-0.0339)\n",
            "\t tensor(3.9765) tensor(2.0066)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0719) tensor(-0.1439)\n",
            "\t tensor(3.9771) tensor(2.0069)\n",
            "\tgrad:  3.0 30.0 tensor(0.0666) tensor(0.1998)\n",
            "\t tensor(3.9786) tensor(2.0084)\n",
            "progress: 128 tensor(0.0001)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0328) tensor(-0.0328)\n",
            "\t tensor(3.9772) tensor(2.0064)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0695) tensor(-0.1391)\n",
            "\t tensor(3.9779) tensor(2.0067)\n",
            "\tgrad:  3.0 30.0 tensor(0.0643) tensor(0.1930)\n",
            "\t tensor(3.9793) tensor(2.0081)\n",
            "progress: 129 tensor(0.0001)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0317) tensor(-0.0317)\n",
            "\t tensor(3.9780) tensor(2.0062)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0672) tensor(-0.1344)\n",
            "\t tensor(3.9786) tensor(2.0065)\n",
            "\tgrad:  3.0 30.0 tensor(0.0622) tensor(0.1867)\n",
            "\t tensor(3.9800) tensor(2.0078)\n",
            "progress: 130 tensor(0.0001)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0306) tensor(-0.0306)\n",
            "\t tensor(3.9787) tensor(2.0060)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0650) tensor(-0.1300)\n",
            "\t tensor(3.9793) tensor(2.0063)\n",
            "\tgrad:  3.0 30.0 tensor(0.0601) tensor(0.1804)\n",
            "\t tensor(3.9806) tensor(2.0076)\n",
            "progress: 131 tensor(0.0001)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0296) tensor(-0.0296)\n",
            "\t tensor(3.9794) tensor(2.0058)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0628) tensor(-0.1256)\n",
            "\t tensor(3.9800) tensor(2.0061)\n",
            "\tgrad:  3.0 30.0 tensor(0.0581) tensor(0.1744)\n",
            "\t tensor(3.9813) tensor(2.0073)\n",
            "progress: 132 tensor(9.3883e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0286) tensor(-0.0286)\n",
            "\t tensor(3.9801) tensor(2.0056)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0607) tensor(-0.1214)\n",
            "\t tensor(3.9807) tensor(2.0059)\n",
            "\tgrad:  3.0 30.0 tensor(0.0562) tensor(0.1686)\n",
            "\t tensor(3.9819) tensor(2.0071)\n",
            "progress: 133 tensor(8.7776e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0277) tensor(-0.0277)\n",
            "\t tensor(3.9808) tensor(2.0054)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0587) tensor(-0.1174)\n",
            "\t tensor(3.9813) tensor(2.0057)\n",
            "\tgrad:  3.0 30.0 tensor(0.0543) tensor(0.1630)\n",
            "\t tensor(3.9825) tensor(2.0068)\n",
            "progress: 134 tensor(8.2013e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0267) tensor(-0.0267)\n",
            "\t tensor(3.9814) tensor(2.0052)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0567) tensor(-0.1135)\n",
            "\t tensor(3.9820) tensor(2.0055)\n",
            "\tgrad:  3.0 30.0 tensor(0.0525) tensor(0.1575)\n",
            "\t tensor(3.9831) tensor(2.0066)\n",
            "progress: 135 tensor(7.6579e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0258) tensor(-0.0258)\n",
            "\t tensor(3.9821) tensor(2.0050)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0548) tensor(-0.1097)\n",
            "\t tensor(3.9826) tensor(2.0053)\n",
            "\tgrad:  3.0 30.0 tensor(0.0508) tensor(0.1523)\n",
            "\t tensor(3.9837) tensor(2.0064)\n",
            "progress: 136 tensor(7.1588e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0250) tensor(-0.0250)\n",
            "\t tensor(3.9826) tensor(2.0049)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0530) tensor(-0.1060)\n",
            "\t tensor(3.9831) tensor(2.0051)\n",
            "\tgrad:  3.0 30.0 tensor(0.0491) tensor(0.1472)\n",
            "\t tensor(3.9842) tensor(2.0062)\n",
            "progress: 137 tensor(6.6891e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0241) tensor(-0.0241)\n",
            "\t tensor(3.9832) tensor(2.0047)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0513) tensor(-0.1025)\n",
            "\t tensor(3.9837) tensor(2.0049)\n",
            "\tgrad:  3.0 30.0 tensor(0.0474) tensor(0.1423)\n",
            "\t tensor(3.9847) tensor(2.0060)\n",
            "progress: 138 tensor(6.2534e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0233) tensor(-0.0233)\n",
            "\t tensor(3.9838) tensor(2.0045)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0495) tensor(-0.0991)\n",
            "\t tensor(3.9843) tensor(2.0048)\n",
            "\tgrad:  3.0 30.0 tensor(0.0459) tensor(0.1376)\n",
            "\t tensor(3.9852) tensor(2.0058)\n",
            "progress: 139 tensor(5.8412e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0226) tensor(-0.0226)\n",
            "\t tensor(3.9843) tensor(2.0044)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0479) tensor(-0.0958)\n",
            "\t tensor(3.9848) tensor(2.0046)\n",
            "\tgrad:  3.0 30.0 tensor(0.0443) tensor(0.1330)\n",
            "\t tensor(3.9857) tensor(2.0056)\n",
            "progress: 140 tensor(5.4570e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0218) tensor(-0.0218)\n",
            "\t tensor(3.9848) tensor(2.0042)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0463) tensor(-0.0926)\n",
            "\t tensor(3.9853) tensor(2.0045)\n",
            "\tgrad:  3.0 30.0 tensor(0.0429) tensor(0.1286)\n",
            "\t tensor(3.9862) tensor(2.0054)\n",
            "progress: 141 tensor(5.1023e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0211) tensor(-0.0211)\n",
            "\t tensor(3.9854) tensor(2.0041)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0448) tensor(-0.0895)\n",
            "\t tensor(3.9858) tensor(2.0043)\n",
            "\tgrad:  3.0 30.0 tensor(0.0414) tensor(0.1242)\n",
            "\t tensor(3.9867) tensor(2.0052)\n",
            "progress: 142 tensor(4.7647e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0204) tensor(-0.0204)\n",
            "\t tensor(3.9858) tensor(2.0040)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0433) tensor(-0.0865)\n",
            "\t tensor(3.9863) tensor(2.0042)\n",
            "\tgrad:  3.0 30.0 tensor(0.0401) tensor(0.1202)\n",
            "\t tensor(3.9871) tensor(2.0050)\n",
            "progress: 143 tensor(4.4565e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0197) tensor(-0.0197)\n",
            "\t tensor(3.9863) tensor(2.0038)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0418) tensor(-0.0836)\n",
            "\t tensor(3.9867) tensor(2.0040)\n",
            "\tgrad:  3.0 30.0 tensor(0.0387) tensor(0.1160)\n",
            "\t tensor(3.9875) tensor(2.0049)\n",
            "progress: 144 tensor(4.1562e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0190) tensor(-0.0190)\n",
            "\t tensor(3.9868) tensor(2.0037)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0404) tensor(-0.0808)\n",
            "\t tensor(3.9872) tensor(2.0039)\n",
            "\tgrad:  3.0 30.0 tensor(0.0374) tensor(0.1123)\n",
            "\t tensor(3.9880) tensor(2.0047)\n",
            "progress: 145 tensor(3.8948e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0184) tensor(-0.0184)\n",
            "\t tensor(3.9872) tensor(2.0036)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0391) tensor(-0.0782)\n",
            "\t tensor(3.9876) tensor(2.0038)\n",
            "\tgrad:  3.0 30.0 tensor(0.0361) tensor(0.1084)\n",
            "\t tensor(3.9884) tensor(2.0045)\n",
            "progress: 146 tensor(3.6281e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0178) tensor(-0.0178)\n",
            "\t tensor(3.9876) tensor(2.0035)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0378) tensor(-0.0755)\n",
            "\t tensor(3.9880) tensor(2.0036)\n",
            "\tgrad:  3.0 30.0 tensor(0.0350) tensor(0.1049)\n",
            "\t tensor(3.9887) tensor(2.0044)\n",
            "progress: 147 tensor(3.3976e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0172) tensor(-0.0172)\n",
            "\t tensor(3.9880) tensor(2.0033)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0365) tensor(-0.0730)\n",
            "\t tensor(3.9884) tensor(2.0035)\n",
            "\tgrad:  3.0 30.0 tensor(0.0338) tensor(0.1014)\n",
            "\t tensor(3.9891) tensor(2.0043)\n",
            "progress: 148 tensor(3.1724e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0166) tensor(-0.0166)\n",
            "\t tensor(3.9884) tensor(2.0032)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0353) tensor(-0.0706)\n",
            "\t tensor(3.9888) tensor(2.0034)\n",
            "\tgrad:  3.0 30.0 tensor(0.0327) tensor(0.0980)\n",
            "\t tensor(3.9895) tensor(2.0041)\n",
            "progress: 149 tensor(2.9632e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0161) tensor(-0.0161)\n",
            "\t tensor(3.9888) tensor(2.0031)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0341) tensor(-0.0682)\n",
            "\t tensor(3.9892) tensor(2.0033)\n",
            "\tgrad:  3.0 30.0 tensor(0.0316) tensor(0.0948)\n",
            "\t tensor(3.9898) tensor(2.0040)\n",
            "progress: 150 tensor(2.7713e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0155) tensor(-0.0155)\n",
            "\t tensor(3.9892) tensor(2.0030)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0330) tensor(-0.0660)\n",
            "\t tensor(3.9895) tensor(2.0032)\n",
            "\tgrad:  3.0 30.0 tensor(0.0305) tensor(0.0916)\n",
            "\t tensor(3.9902) tensor(2.0038)\n",
            "progress: 151 tensor(2.5896e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0150) tensor(-0.0150)\n",
            "\t tensor(3.9896) tensor(2.0029)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0319) tensor(-0.0638)\n",
            "\t tensor(3.9899) tensor(2.0031)\n",
            "\tgrad:  3.0 30.0 tensor(0.0295) tensor(0.0886)\n",
            "\t tensor(3.9905) tensor(2.0037)\n",
            "progress: 152 tensor(2.4216e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0145) tensor(-0.0145)\n",
            "\t tensor(3.9899) tensor(2.0028)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0308) tensor(-0.0616)\n",
            "\t tensor(3.9902) tensor(2.0030)\n",
            "\tgrad:  3.0 30.0 tensor(0.0285) tensor(0.0856)\n",
            "\t tensor(3.9908) tensor(2.0036)\n",
            "progress: 153 tensor(2.2592e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0140) tensor(-0.0140)\n",
            "\t tensor(3.9902) tensor(2.0027)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0298) tensor(-0.0596)\n",
            "\t tensor(3.9905) tensor(2.0029)\n",
            "\tgrad:  3.0 30.0 tensor(0.0276) tensor(0.0828)\n",
            "\t tensor(3.9911) tensor(2.0035)\n",
            "progress: 154 tensor(2.1165e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0136) tensor(-0.0136)\n",
            "\t tensor(3.9906) tensor(2.0026)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0288) tensor(-0.0576)\n",
            "\t tensor(3.9908) tensor(2.0028)\n",
            "\tgrad:  3.0 30.0 tensor(0.0266) tensor(0.0799)\n",
            "\t tensor(3.9914) tensor(2.0034)\n",
            "progress: 155 tensor(1.9716e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0131) tensor(-0.0131)\n",
            "\t tensor(3.9909) tensor(2.0026)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0278) tensor(-0.0557)\n",
            "\t tensor(3.9912) tensor(2.0027)\n",
            "\tgrad:  3.0 30.0 tensor(0.0258) tensor(0.0774)\n",
            "\t tensor(3.9917) tensor(2.0032)\n",
            "progress: 156 tensor(1.8483e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0127) tensor(-0.0127)\n",
            "\t tensor(3.9912) tensor(2.0025)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0269) tensor(-0.0538)\n",
            "\t tensor(3.9914) tensor(2.0026)\n",
            "\tgrad:  3.0 30.0 tensor(0.0249) tensor(0.0747)\n",
            "\t tensor(3.9920) tensor(2.0031)\n",
            "progress: 157 tensor(1.7226e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0123) tensor(-0.0123)\n",
            "\t tensor(3.9915) tensor(2.0024)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0260) tensor(-0.0520)\n",
            "\t tensor(3.9917) tensor(2.0025)\n",
            "\tgrad:  3.0 30.0 tensor(0.0241) tensor(0.0722)\n",
            "\t tensor(3.9923) tensor(2.0030)\n",
            "progress: 158 tensor(1.6105e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0118) tensor(-0.0118)\n",
            "\t tensor(3.9918) tensor(2.0023)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0251) tensor(-0.0503)\n",
            "\t tensor(3.9920) tensor(2.0024)\n",
            "\tgrad:  3.0 30.0 tensor(0.0233) tensor(0.0698)\n",
            "\t tensor(3.9925) tensor(2.0029)\n",
            "progress: 159 tensor(1.5051e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0115) tensor(-0.0115)\n",
            "\t tensor(3.9920) tensor(2.0022)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0243) tensor(-0.0486)\n",
            "\t tensor(3.9923) tensor(2.0023)\n",
            "\tgrad:  3.0 30.0 tensor(0.0225) tensor(0.0675)\n",
            "\t tensor(3.9928) tensor(2.0028)\n",
            "progress: 160 tensor(1.4061e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0111) tensor(-0.0111)\n",
            "\t tensor(3.9923) tensor(2.0022)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0235) tensor(-0.0470)\n",
            "\t tensor(3.9925) tensor(2.0023)\n",
            "\tgrad:  3.0 30.0 tensor(0.0217) tensor(0.0652)\n",
            "\t tensor(3.9930) tensor(2.0027)\n",
            "progress: 161 tensor(1.3133e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0107) tensor(-0.0107)\n",
            "\t tensor(3.9926) tensor(2.0021)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0227) tensor(-0.0454)\n",
            "\t tensor(3.9928) tensor(2.0022)\n",
            "\tgrad:  3.0 30.0 tensor(0.0210) tensor(0.0631)\n",
            "\t tensor(3.9932) tensor(2.0026)\n",
            "progress: 162 tensor(1.2290e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0103) tensor(-0.0103)\n",
            "\t tensor(3.9928) tensor(2.0020)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0220) tensor(-0.0439)\n",
            "\t tensor(3.9930) tensor(2.0021)\n",
            "\tgrad:  3.0 30.0 tensor(0.0203) tensor(0.0610)\n",
            "\t tensor(3.9935) tensor(2.0026)\n",
            "progress: 163 tensor(1.1475e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0100) tensor(-0.0100)\n",
            "\t tensor(3.9931) tensor(2.0019)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0212) tensor(-0.0424)\n",
            "\t tensor(3.9933) tensor(2.0020)\n",
            "\tgrad:  3.0 30.0 tensor(0.0196) tensor(0.0589)\n",
            "\t tensor(3.9937) tensor(2.0025)\n",
            "progress: 164 tensor(1.0713e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0097) tensor(-0.0097)\n",
            "\t tensor(3.9933) tensor(2.0019)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0205) tensor(-0.0410)\n",
            "\t tensor(3.9935) tensor(2.0020)\n",
            "\tgrad:  3.0 30.0 tensor(0.0190) tensor(0.0570)\n",
            "\t tensor(3.9939) tensor(2.0024)\n",
            "progress: 165 tensor(1.0013e-05)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0093) tensor(-0.0093)\n",
            "\t tensor(3.9935) tensor(2.0018)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0198) tensor(-0.0397)\n",
            "\t tensor(3.9937) tensor(2.0019)\n",
            "\tgrad:  3.0 30.0 tensor(0.0184) tensor(0.0551)\n",
            "\t tensor(3.9941) tensor(2.0023)\n",
            "progress: 166 tensor(9.3599e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0090) tensor(-0.0090)\n",
            "\t tensor(3.9937) tensor(2.0018)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0192) tensor(-0.0383)\n",
            "\t tensor(3.9939) tensor(2.0018)\n",
            "\tgrad:  3.0 30.0 tensor(0.0177) tensor(0.0532)\n",
            "\t tensor(3.9943) tensor(2.0022)\n",
            "progress: 167 tensor(8.7402e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0087) tensor(-0.0087)\n",
            "\t tensor(3.9939) tensor(2.0017)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0185) tensor(-0.0371)\n",
            "\t tensor(3.9941) tensor(2.0018)\n",
            "\tgrad:  3.0 30.0 tensor(0.0172) tensor(0.0515)\n",
            "\t tensor(3.9945) tensor(2.0022)\n",
            "progress: 168 tensor(8.1855e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0084) tensor(-0.0084)\n",
            "\t tensor(3.9941) tensor(2.0016)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0179) tensor(-0.0358)\n",
            "\t tensor(3.9943) tensor(2.0017)\n",
            "\tgrad:  3.0 30.0 tensor(0.0166) tensor(0.0497)\n",
            "\t tensor(3.9947) tensor(2.0021)\n",
            "progress: 169 tensor(7.6383e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0082) tensor(-0.0082)\n",
            "\t tensor(3.9943) tensor(2.0016)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0173) tensor(-0.0347)\n",
            "\t tensor(3.9945) tensor(2.0017)\n",
            "\tgrad:  3.0 30.0 tensor(0.0160) tensor(0.0481)\n",
            "\t tensor(3.9948) tensor(2.0020)\n",
            "progress: 170 tensor(7.1304e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0079) tensor(-0.0079)\n",
            "\t tensor(3.9945) tensor(2.0015)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0167) tensor(-0.0335)\n",
            "\t tensor(3.9947) tensor(2.0016)\n",
            "\tgrad:  3.0 30.0 tensor(0.0155) tensor(0.0465)\n",
            "\t tensor(3.9950) tensor(2.0019)\n",
            "progress: 171 tensor(6.6696e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0076) tensor(-0.0076)\n",
            "\t tensor(3.9947) tensor(2.0015)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0162) tensor(-0.0324)\n",
            "\t tensor(3.9949) tensor(2.0016)\n",
            "\tgrad:  3.0 30.0 tensor(0.0150) tensor(0.0449)\n",
            "\t tensor(3.9952) tensor(2.0019)\n",
            "progress: 172 tensor(6.2241e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0074) tensor(-0.0074)\n",
            "\t tensor(3.9949) tensor(2.0014)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0156) tensor(-0.0313)\n",
            "\t tensor(3.9950) tensor(2.0015)\n",
            "\tgrad:  3.0 30.0 tensor(0.0145) tensor(0.0435)\n",
            "\t tensor(3.9953) tensor(2.0018)\n",
            "progress: 173 tensor(5.8308e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0071) tensor(-0.0071)\n",
            "\t tensor(3.9951) tensor(2.0014)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0151) tensor(-0.0303)\n",
            "\t tensor(3.9952) tensor(2.0015)\n",
            "\tgrad:  3.0 30.0 tensor(0.0140) tensor(0.0420)\n",
            "\t tensor(3.9955) tensor(2.0018)\n",
            "progress: 174 tensor(5.4414e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0069) tensor(-0.0069)\n",
            "\t tensor(3.9952) tensor(2.0013)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0146) tensor(-0.0292)\n",
            "\t tensor(3.9954) tensor(2.0014)\n",
            "\tgrad:  3.0 30.0 tensor(0.0135) tensor(0.0406)\n",
            "\t tensor(3.9956) tensor(2.0017)\n",
            "progress: 175 tensor(5.0827e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0067) tensor(-0.0067)\n",
            "\t tensor(3.9954) tensor(2.0013)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0141) tensor(-0.0283)\n",
            "\t tensor(3.9955) tensor(2.0014)\n",
            "\tgrad:  3.0 30.0 tensor(0.0131) tensor(0.0392)\n",
            "\t tensor(3.9958) tensor(2.0016)\n",
            "progress: 176 tensor(4.7445e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0064) tensor(-0.0064)\n",
            "\t tensor(3.9955) tensor(2.0013)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0136) tensor(-0.0273)\n",
            "\t tensor(3.9957) tensor(2.0013)\n",
            "\tgrad:  3.0 30.0 tensor(0.0127) tensor(0.0380)\n",
            "\t tensor(3.9959) tensor(2.0016)\n",
            "progress: 177 tensor(4.4501e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0062) tensor(-0.0062)\n",
            "\t tensor(3.9957) tensor(2.0012)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0132) tensor(-0.0264)\n",
            "\t tensor(3.9958) tensor(2.0013)\n",
            "\tgrad:  3.0 30.0 tensor(0.0122) tensor(0.0366)\n",
            "\t tensor(3.9961) tensor(2.0015)\n",
            "progress: 178 tensor(4.1418e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0060) tensor(-0.0060)\n",
            "\t tensor(3.9958) tensor(2.0012)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0128) tensor(-0.0255)\n",
            "\t tensor(3.9959) tensor(2.0012)\n",
            "\tgrad:  3.0 30.0 tensor(0.0118) tensor(0.0354)\n",
            "\t tensor(3.9962) tensor(2.0015)\n",
            "progress: 179 tensor(3.8745e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0058) tensor(-0.0058)\n",
            "\t tensor(3.9960) tensor(2.0011)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0123) tensor(-0.0247)\n",
            "\t tensor(3.9961) tensor(2.0012)\n",
            "\tgrad:  3.0 30.0 tensor(0.0114) tensor(0.0343)\n",
            "\t tensor(3.9963) tensor(2.0014)\n",
            "progress: 180 tensor(3.6307e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0056) tensor(-0.0056)\n",
            "\t tensor(3.9961) tensor(2.0011)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0119) tensor(-0.0239)\n",
            "\t tensor(3.9962) tensor(2.0011)\n",
            "\tgrad:  3.0 30.0 tensor(0.0110) tensor(0.0331)\n",
            "\t tensor(3.9964) tensor(2.0014)\n",
            "progress: 181 tensor(3.3808e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0054) tensor(-0.0054)\n",
            "\t tensor(3.9962) tensor(2.0011)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0115) tensor(-0.0230)\n",
            "\t tensor(3.9963) tensor(2.0011)\n",
            "\tgrad:  3.0 30.0 tensor(0.0107) tensor(0.0321)\n",
            "\t tensor(3.9966) tensor(2.0013)\n",
            "progress: 182 tensor(3.1736e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0053) tensor(-0.0053)\n",
            "\t tensor(3.9964) tensor(2.0010)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0112) tensor(-0.0223)\n",
            "\t tensor(3.9965) tensor(2.0011)\n",
            "\tgrad:  3.0 30.0 tensor(0.0103) tensor(0.0309)\n",
            "\t tensor(3.9967) tensor(2.0013)\n",
            "progress: 183 tensor(2.9533e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0051) tensor(-0.0051)\n",
            "\t tensor(3.9965) tensor(2.0010)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0108) tensor(-0.0215)\n",
            "\t tensor(3.9966) tensor(2.0010)\n",
            "\tgrad:  3.0 30.0 tensor(0.0100) tensor(0.0299)\n",
            "\t tensor(3.9968) tensor(2.0013)\n",
            "progress: 184 tensor(2.7663e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0049) tensor(-0.0049)\n",
            "\t tensor(3.9966) tensor(2.0010)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0104) tensor(-0.0208)\n",
            "\t tensor(3.9967) tensor(2.0010)\n",
            "\tgrad:  3.0 30.0 tensor(0.0096) tensor(0.0289)\n",
            "\t tensor(3.9969) tensor(2.0012)\n",
            "progress: 185 tensor(2.5792e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0047) tensor(-0.0047)\n",
            "\t tensor(3.9967) tensor(2.0009)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0101) tensor(-0.0201)\n",
            "\t tensor(3.9968) tensor(2.0010)\n",
            "\tgrad:  3.0 30.0 tensor(0.0093) tensor(0.0279)\n",
            "\t tensor(3.9970) tensor(2.0012)\n",
            "progress: 186 tensor(2.4105e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0046) tensor(-0.0046)\n",
            "\t tensor(3.9968) tensor(2.0009)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0097) tensor(-0.0195)\n",
            "\t tensor(3.9969) tensor(2.0009)\n",
            "\tgrad:  3.0 30.0 tensor(0.0090) tensor(0.0270)\n",
            "\t tensor(3.9971) tensor(2.0011)\n",
            "progress: 187 tensor(2.2533e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0044) tensor(-0.0044)\n",
            "\t tensor(3.9969) tensor(2.0009)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0094) tensor(-0.0188)\n",
            "\t tensor(3.9970) tensor(2.0009)\n",
            "\tgrad:  3.0 30.0 tensor(0.0087) tensor(0.0261)\n",
            "\t tensor(3.9972) tensor(2.0011)\n",
            "progress: 188 tensor(2.1068e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0043) tensor(-0.0043)\n",
            "\t tensor(3.9970) tensor(2.0008)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0091) tensor(-0.0182)\n",
            "\t tensor(3.9971) tensor(2.0009)\n",
            "\tgrad:  3.0 30.0 tensor(0.0084) tensor(0.0253)\n",
            "\t tensor(3.9973) tensor(2.0011)\n",
            "progress: 189 tensor(1.9707e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0041) tensor(-0.0041)\n",
            "\t tensor(3.9971) tensor(2.0008)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0088) tensor(-0.0176)\n",
            "\t tensor(3.9972) tensor(2.0008)\n",
            "\tgrad:  3.0 30.0 tensor(0.0081) tensor(0.0244)\n",
            "\t tensor(3.9974) tensor(2.0010)\n",
            "progress: 190 tensor(1.8443e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0040) tensor(-0.0040)\n",
            "\t tensor(3.9972) tensor(2.0008)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0085) tensor(-0.0170)\n",
            "\t tensor(3.9973) tensor(2.0008)\n",
            "\tgrad:  3.0 30.0 tensor(0.0079) tensor(0.0236)\n",
            "\t tensor(3.9975) tensor(2.0010)\n",
            "progress: 191 tensor(1.7120e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0039) tensor(-0.0039)\n",
            "\t tensor(3.9973) tensor(2.0008)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0082) tensor(-0.0164)\n",
            "\t tensor(3.9974) tensor(2.0008)\n",
            "\tgrad:  3.0 30.0 tensor(0.0076) tensor(0.0229)\n",
            "\t tensor(3.9976) tensor(2.0010)\n",
            "progress: 192 tensor(1.6136e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0037) tensor(-0.0037)\n",
            "\t tensor(3.9974) tensor(2.0007)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0079) tensor(-0.0159)\n",
            "\t tensor(3.9975) tensor(2.0008)\n",
            "\tgrad:  3.0 30.0 tensor(0.0073) tensor(0.0220)\n",
            "\t tensor(3.9976) tensor(2.0009)\n",
            "progress: 193 tensor(1.4994e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0036) tensor(-0.0036)\n",
            "\t tensor(3.9975) tensor(2.0007)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0077) tensor(-0.0154)\n",
            "\t tensor(3.9976) tensor(2.0007)\n",
            "\tgrad:  3.0 30.0 tensor(0.0071) tensor(0.0214)\n",
            "\t tensor(3.9977) tensor(2.0009)\n",
            "progress: 194 tensor(1.4075e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0035) tensor(-0.0035)\n",
            "\t tensor(3.9976) tensor(2.0007)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0074) tensor(-0.0148)\n",
            "\t tensor(3.9976) tensor(2.0007)\n",
            "\tgrad:  3.0 30.0 tensor(0.0069) tensor(0.0206)\n",
            "\t tensor(3.9978) tensor(2.0009)\n",
            "progress: 195 tensor(1.3097e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0034) tensor(-0.0034)\n",
            "\t tensor(3.9977) tensor(2.0007)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0072) tensor(-0.0143)\n",
            "\t tensor(3.9977) tensor(2.0007)\n",
            "\tgrad:  3.0 30.0 tensor(0.0066) tensor(0.0199)\n",
            "\t tensor(3.9979) tensor(2.0008)\n",
            "progress: 196 tensor(1.2280e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0033) tensor(-0.0033)\n",
            "\t tensor(3.9977) tensor(2.0006)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0069) tensor(-0.0139)\n",
            "\t tensor(3.9978) tensor(2.0007)\n",
            "\tgrad:  3.0 30.0 tensor(0.0064) tensor(0.0192)\n",
            "\t tensor(3.9979) tensor(2.0008)\n",
            "progress: 197 tensor(1.1409e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0032) tensor(-0.0032)\n",
            "\t tensor(3.9978) tensor(2.0006)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0067) tensor(-0.0134)\n",
            "\t tensor(3.9979) tensor(2.0006)\n",
            "\tgrad:  3.0 30.0 tensor(0.0062) tensor(0.0186)\n",
            "\t tensor(3.9980) tensor(2.0008)\n",
            "progress: 198 tensor(1.0687e-06)\n",
            "\tgrad:  1.0 6.0 tensor(-0.0031) tensor(-0.0031)\n",
            "\t tensor(3.9979) tensor(2.0006)\n",
            "\tgrad:  2.0 16.0 tensor(-0.0065) tensor(-0.0130)\n",
            "\t tensor(3.9979) tensor(2.0006)\n",
            "\tgrad:  3.0 30.0 tensor(0.0060) tensor(0.0180)\n",
            "\t tensor(3.9981) tensor(2.0008)\n",
            "progress: 199 tensor(1.0027e-06)\n",
            "predict (after training) 4 hours tensor(48.0010)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atwru4swKAJI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}